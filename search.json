[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Kevin Andrew Silberberg, a recent graduate from the Scientific Computing and Applied Math Master’s program at University of California Santa Cruz.\n\n\n\nThis website serves as my consolidated online presence, moving away from mainstream social media as the primary medium of my digital identity. I’ve designed it to be a focused space for learning, documenting, and sharing my work and thoughts.\nEvery document accessible on this website was created using Quarto markdown in an Emacs environment and published on GitHub Pages. I’ve found this workflow particularly rewarding for technical documentation, coding projects, and academic work.\n\n\n\nStream: My personal blog where I share thoughts, projects, and insights\nPhotos: A visual blog featuring photography from my travels and daily life, with dedicated sections for specific trips and a continuous stream of recent photos\n\n\n\n\n\nEmail: ksilberb@pm.me\nPhone: 1 (619) 417-6705\nWebsite: ksilberb.github.io/paxnomada\n\n\n\nGitHub\nORCID\nLinkedIn\n\n\n\n\n\nBlueSky\nInstagram\nFacebook"
  },
  {
    "objectID": "about.html#introduction",
    "href": "about.html#introduction",
    "title": "About",
    "section": "",
    "text": "I’m Kevin Andrew Silberberg, a recent graduate from the Scientific Computing and Applied Math Master’s program at University of California Santa Cruz."
  },
  {
    "objectID": "about.html#about-this-website",
    "href": "about.html#about-this-website",
    "title": "About",
    "section": "",
    "text": "This website serves as my consolidated online presence, moving away from mainstream social media as the primary medium of my digital identity. I’ve designed it to be a focused space for learning, documenting, and sharing my work and thoughts.\nEvery document accessible on this website was created using Quarto markdown in an Emacs environment and published on GitHub Pages. I’ve found this workflow particularly rewarding for technical documentation, coding projects, and academic work.\n\n\n\nStream: My personal blog where I share thoughts, projects, and insights\nPhotos: A visual blog featuring photography from my travels and daily life, with dedicated sections for specific trips and a continuous stream of recent photos"
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "About",
    "section": "",
    "text": "Email: ksilberb@pm.me\nPhone: 1 (619) 417-6705\nWebsite: ksilberb.github.io/paxnomada\n\n\n\nGitHub\nORCID\nLinkedIn\n\n\n\n\n\nBlueSky\nInstagram\nFacebook"
  },
  {
    "objectID": "photos/shasta2023/index.html",
    "href": "photos/shasta2023/index.html",
    "title": "Shasta 2023",
    "section": "",
    "text": "This was a trip that Ben Ruef and I took right after finals week of Spring 2023. We road triped from Santa Cruz, to Mt. Shasta, then we stopped in Ashland Oregon while on our way to Promontory in Northwest California.\n\n\n\n\nSat, 17 Jun 2023, 11:18:55 AM PDT\n\n\n\n\nThe view of Mt. Shasta from the trail, on our way up to a climb at Castle Crags.\n\n\n\n\n\nSat, 17 Jun 2023, 03:26:33 PM PDT\n\n\n\n\nThe top of a small scramble.\n\n\n\n\n\n\n\n\nTue, 20 Jun 2023, 05:52:29 PM PDT\n\n\n\n\nOn our way up to Helen Lake. :)\n\n\n\n\n\nTue, 20 Jun 2023, 08:04:17 PM PDT\n\n\n\n\nMaking lap wraps inside the tent. We hung all our wet socks and boots from the hike up.\n\n\n\n\n\nTue, 20 Jun 2023, 08:53:38 PM PDT\n\n\n\n\nA photo I snapped after we got settled in and ate.\n\n\n\n\n\nWed, 21 Jun 2023, 06:46:24 AM PDT\n\n\n\n\nWhen we left Helen Lake at 4 am I melted some snow into a nalgene and I think it pretty much froze solid at this point.\n\n\n\n\n\nWed, 21 Jun 2023, 08:34:54 AM PDT\n\n\n\n\nThe Red Banks.\n\n\n\n\n\n\n\n\nSat, 24 Jun 2023, 11:05:20 AM PDT\n\n\n\n\nWe ran into some friends from pacific edge at promontory\n\n\n\nback to top"
  },
  {
    "objectID": "photos/shasta2023/index.html#castle-crags",
    "href": "photos/shasta2023/index.html#castle-crags",
    "title": "Shasta 2023",
    "section": "",
    "text": "Sat, 17 Jun 2023, 11:18:55 AM PDT\n\n\n\n\nThe view of Mt. Shasta from the trail, on our way up to a climb at Castle Crags.\n\n\n\n\n\nSat, 17 Jun 2023, 03:26:33 PM PDT\n\n\n\n\nThe top of a small scramble."
  },
  {
    "objectID": "photos/shasta2023/index.html#mt.-shasta",
    "href": "photos/shasta2023/index.html#mt.-shasta",
    "title": "Shasta 2023",
    "section": "",
    "text": "Tue, 20 Jun 2023, 05:52:29 PM PDT\n\n\n\n\nOn our way up to Helen Lake. :)\n\n\n\n\n\nTue, 20 Jun 2023, 08:04:17 PM PDT\n\n\n\n\nMaking lap wraps inside the tent. We hung all our wet socks and boots from the hike up.\n\n\n\n\n\nTue, 20 Jun 2023, 08:53:38 PM PDT\n\n\n\n\nA photo I snapped after we got settled in and ate.\n\n\n\n\n\nWed, 21 Jun 2023, 06:46:24 AM PDT\n\n\n\n\nWhen we left Helen Lake at 4 am I melted some snow into a nalgene and I think it pretty much froze solid at this point.\n\n\n\n\n\nWed, 21 Jun 2023, 08:34:54 AM PDT\n\n\n\n\nThe Red Banks."
  },
  {
    "objectID": "photos/shasta2023/index.html#promontory",
    "href": "photos/shasta2023/index.html#promontory",
    "title": "Shasta 2023",
    "section": "",
    "text": "Sat, 24 Jun 2023, 11:05:20 AM PDT\n\n\n\n\nWe ran into some friends from pacific edge at promontory\n\n\n\nback to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin Andrew Silberberg",
    "section": "",
    "text": "Hi, this is my personal website. Please see the about page for more information.\n\n\nFundamentals of Uncertainty Quantification\nAdvanced Methods in Applied Mathematics\n\n\n\nRosalind\nProject Euler"
  },
  {
    "objectID": "index.html#courses-taken",
    "href": "index.html#courses-taken",
    "title": "Kevin Andrew Silberberg",
    "section": "",
    "text": "Fundamentals of Uncertainty Quantification\nAdvanced Methods in Applied Mathematics"
  },
  {
    "objectID": "index.html#problem-sets",
    "href": "index.html#problem-sets",
    "title": "Kevin Andrew Silberberg",
    "section": "",
    "text": "Rosalind\nProject Euler"
  },
  {
    "objectID": "projecteuler/largestprimefactor.html",
    "href": "projecteuler/largestprimefactor.html",
    "title": "Largest Prime Factor (Problem 3)",
    "section": "",
    "text": "The prime factors of 13195 are 5, 7, 13, and 29.\nWhat is the largest prime factor of the number 600851475143?\n\n\nA prime number is a whole number above \\(1\\) that cannot be made by multiplying other whole numbers.\nFor this problem we are going to need to generate prime numbers which I do not know how to do.\nresearch\nGeneration of primes (wiki)\nSieve of Atkin\nLet us implement the Sieve of Atkin algorithm, so we can generate prime numbers.\n\nfunction atkinSieve(N::Int)\n    sieve = falses(N)\n    if N &gt;= 2 sieve[2] = true end\n    if N &gt;= 3 sieve[3] = true end\n    if N &gt;= 5 sieve[5] = true end\n\n    for x in 1:isqrt(N)\n        for y in 1:isqrt(N)\n            n = 4*x*x + y*y\n            if n &lt;= N && (mod(n, 12) == 1 || mod(n, 12) == 5)\n                sieve[n] = !sieve[n]\n            end\n            n = 3*x*x + y*y\n            if n &lt;= N && mod(n, 12) == 7\n                sieve[n] = !sieve[n]\n            end\n            if x &gt; y\n                n = 3*x*x - y*y\n                if n &lt;= N && mod(n, 12) == 11\n                    sieve[n] = !sieve[n]\n                end\n            end\n        end\n    end\n\n    for r in 5:isqrt(N)\n        if sieve[r]\n            for i in r*r:r*r:N\n                sieve[i] = false\n            end\n        end\n    end\n    [i for i in 1:N if sieve[i]]\nend\n\natkinSieve (generic function with 1 method)\n\n\nFor the next part of the problem we need to develope an algorithm that finds the prime factorization for any integer \\(K\\).\n\nfunction primeFactor(K::Int)\n    if K &lt;= 0\n        throw(DomainError(K, \"Input must be a positive integer\"))\n    elseif K == 1\n        return Dict{Int, Int}()\n    end\n    # generate prime numbers up to sqrt(K)+1\n    primes = atkinSieve(isqrt(K) + 1)\n    factors = Dict{Int, Int}()\n    remaining = K\n    for p in primes\n        if remaining == 1\n            break\n        end\n        power = 0\n        # count how many times p divides remaining\n        while mod(remaining, p) == 0\n            power += 1\n            remaining ÷= p\n        end\n        # add the prime factor to dict\n        # the value represents how many times that factor divided remaining\n        if power &gt; 0\n            factors[p]  = power\n        end\n    end\n    # if the remaining value is greater than 1, it must be prime\n    # if it were composite it would have at least one prime factor less than or equal to sqrt(K)\n    # which we would have diveded out already.\n    if remaining &gt; 1\n        factors[remaining] = 1\n    end\n    factors\nend\nprimeFactor(600851475143)\n\nDict{Int64, Int64} with 4 entries:\n  839  =&gt; 1\n  71   =&gt; 1\n  1471 =&gt; 1\n  6857 =&gt; 1\n\n\nSo the largest prime number is therefore \\(6857\\)"
  },
  {
    "objectID": "projecteuler/largestprimefactor.html#solution",
    "href": "projecteuler/largestprimefactor.html#solution",
    "title": "Largest Prime Factor (Problem 3)",
    "section": "",
    "text": "A prime number is a whole number above \\(1\\) that cannot be made by multiplying other whole numbers.\nFor this problem we are going to need to generate prime numbers which I do not know how to do.\nresearch\nGeneration of primes (wiki)\nSieve of Atkin\nLet us implement the Sieve of Atkin algorithm, so we can generate prime numbers.\n\nfunction atkinSieve(N::Int)\n    sieve = falses(N)\n    if N &gt;= 2 sieve[2] = true end\n    if N &gt;= 3 sieve[3] = true end\n    if N &gt;= 5 sieve[5] = true end\n\n    for x in 1:isqrt(N)\n        for y in 1:isqrt(N)\n            n = 4*x*x + y*y\n            if n &lt;= N && (mod(n, 12) == 1 || mod(n, 12) == 5)\n                sieve[n] = !sieve[n]\n            end\n            n = 3*x*x + y*y\n            if n &lt;= N && mod(n, 12) == 7\n                sieve[n] = !sieve[n]\n            end\n            if x &gt; y\n                n = 3*x*x - y*y\n                if n &lt;= N && mod(n, 12) == 11\n                    sieve[n] = !sieve[n]\n                end\n            end\n        end\n    end\n\n    for r in 5:isqrt(N)\n        if sieve[r]\n            for i in r*r:r*r:N\n                sieve[i] = false\n            end\n        end\n    end\n    [i for i in 1:N if sieve[i]]\nend\n\natkinSieve (generic function with 1 method)\n\n\nFor the next part of the problem we need to develope an algorithm that finds the prime factorization for any integer \\(K\\).\n\nfunction primeFactor(K::Int)\n    if K &lt;= 0\n        throw(DomainError(K, \"Input must be a positive integer\"))\n    elseif K == 1\n        return Dict{Int, Int}()\n    end\n    # generate prime numbers up to sqrt(K)+1\n    primes = atkinSieve(isqrt(K) + 1)\n    factors = Dict{Int, Int}()\n    remaining = K\n    for p in primes\n        if remaining == 1\n            break\n        end\n        power = 0\n        # count how many times p divides remaining\n        while mod(remaining, p) == 0\n            power += 1\n            remaining ÷= p\n        end\n        # add the prime factor to dict\n        # the value represents how many times that factor divided remaining\n        if power &gt; 0\n            factors[p]  = power\n        end\n    end\n    # if the remaining value is greater than 1, it must be prime\n    # if it were composite it would have at least one prime factor less than or equal to sqrt(K)\n    # which we would have diveded out already.\n    if remaining &gt; 1\n        factors[remaining] = 1\n    end\n    factors\nend\nprimeFactor(600851475143)\n\nDict{Int64, Int64} with 4 entries:\n  839  =&gt; 1\n  71   =&gt; 1\n  1471 =&gt; 1\n  6857 =&gt; 1\n\n\nSo the largest prime number is therefore \\(6857\\)"
  },
  {
    "objectID": "projecteuler/index.html",
    "href": "projecteuler/index.html",
    "title": "Project Euler",
    "section": "",
    "text": "Project Euler is a website with problems in mathematics and computer science. The following are my own solutions to the problems.\n\n\nMultiples of 3 or 5\nEven Fibonacci Numbers\nLargest Prime Factor\nLargest Palindrome Product"
  },
  {
    "objectID": "projecteuler/index.html#solutions",
    "href": "projecteuler/index.html#solutions",
    "title": "Project Euler",
    "section": "",
    "text": "Multiples of 3 or 5\nEven Fibonacci Numbers\nLargest Prime Factor\nLargest Palindrome Product"
  },
  {
    "objectID": "projecteuler/smallestmultiple.html",
    "href": "projecteuler/smallestmultiple.html",
    "title": "Smallest Multiple",
    "section": "",
    "text": "\\(2520\\) is the smallest number that can be divided by each of the numbers from \\(1\\) to \\(10\\) without any remainder.\nWhat is the smallest positive number that can is evenly divisible by all of the numbers from \\(1\\) to \\(20\\)?\n\n\n\nfunction isDivisible(num::Int)\n    r::Int = 0\n    for i = 1:20\n        r = mod(num, i)\n        if r != 0\n            return false\n        end\n    end\n    return true\nend\n\nfunction main()\n    current = 2520\n    while !(isDivisible(current))\n        current += 1\n    end\n    current\nend\nmain()\n\n232792560"
  },
  {
    "objectID": "projecteuler/smallestmultiple.html#solution",
    "href": "projecteuler/smallestmultiple.html#solution",
    "title": "Smallest Multiple",
    "section": "",
    "text": "function isDivisible(num::Int)\n    r::Int = 0\n    for i = 1:20\n        r = mod(num, i)\n        if r != 0\n            return false\n        end\n    end\n    return true\nend\n\nfunction main()\n    current = 2520\n    while !(isDivisible(current))\n        current += 1\n    end\n    current\nend\nmain()\n\n232792560"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw1/hw1.html",
    "href": "courses/uncertainty_quant/docs/hw1/hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Let \\(X\\) be a random variable with Probability density function:\n\\[p(x) =\n    \\begin{cases}\n        \\frac{2x\\cos{(x^2)} + 5}{10 + \\sin{(4)}} & x \\in [0, 2] \\\\\n        0 & \\text{otherwise}\n    \\end{cases} \\tag{1}\\]\n\nusing CairoMakie\n\nfunction p(x::Real)\n    (0 ≤ x ≤ 2.0) ? (2.0 * x * cos(x^2.0) + 5.0) / (10.0 + sin(4.0)) : 0.0\nend\n\nfunction plotPDF()\n    fig = Figure()\n    ax = Axis(fig[1,1],\n               title = \"PDF\",\n               xlabel = \"x\",\n               ylabel = \"p(x)\"\n               )\n    r = -0.1:0.01:2.1\n    lines!(ax, r, p.(r))\n    xlims!(ax, -0.1, 2.1)\n    ylims!(ax, -0.1, 0.7)\n    display(fig);\nend\nplotPDF();\n\n\n\n\n\n\n\n\nUsing the Gauss-Kronrod quadrature we can numerically solve for the mean and the variance by calculating the first moment and second momements of the PDF of X.\n\nusing QuadGK\n\nfunction E1(x::Real)\n    return x * p(x)\nend\n\nfunction E2(x::Real)\n    return x^2 * p(x)\nend\n\nfunction parta_numeric()\n    Ex  = quadgk(E1, 0.0, 2.0, rtol=1e-3)[1]\n    Ex² = quadgk(E2, 0.0, 2.0, rtol=1e-3)[1]\n    σ²  = Ex² - (Ex)^2\n    σ   = sqrt(σ²)\n    println(\"The numeric mean and standard deviation of the PDF of X are:\")\n    println(\"μ = $Ex\")\n    println(\"σ = $σ\")\n    return Ex, σ\nend\n\nμ_numeric, σ_numeric = parta_numeric();\n\nThe numeric mean and standard deviation of the PDF of X are:\nμ = 0.8310564083631247\nσ = 0.4954158522588331\n\n\nLet us solve for the first and second moments of (Equation 1) analytically and compare with the numerical findings.\n\n\n\n\\[\\begin{align}\n    E[X] &= \\int_0^2 x\\bigg(\\frac{2x\\cos{(x^2)} + 5}{10 + \\sin{(4)}}\\bigg)dx \\\\\n    &= x\\bigg(\\frac{\\sin{(x^2)} + 5x}{10 + \\sin{(4)}}\\bigg)\\bigg{\\vert}_0^2 - \\frac{1}{10 + \\sin{(4)}}\\int_0^2\\bigg(\\sin{(x^2)} + 5x\\bigg)dx \\\\\n    &= \\frac{-\\sqrt{\\frac{\\pi}{2}}S\\big(\\sqrt{\\frac{2}{\\pi}}x\\big) + \\frac{5x^2}{2} + x\\sin{(x^2)}}{10 + \\sin{(4)}}\\bigg{\\vert}_0^2 \\\\\n\\end{align}\\]\nWhere \\(S(x)\\) is the fresnel integral defined as \\[S(x) = \\int_0^x \\sin{(t^2)} dt\\]\n\nusing FresnelIntegrals\nfunction first_moment()\n    E(x) = (-sqrt(π/2.0)*fresnels(sqrt(2.0/π)*x) + (5.0/2.0)*x^2 + x*sin(x^2.0)) / (10.0 + sin(4.0))\n    μ = E(2.0) - E(0.0)\nend\nμ_exact = first_moment();\n\n\n\n\n\\[\\begin{align}\n    E[X^2] &= \\int_0^2x^2\\bigg(\\frac{2x\\cos{(x^2)}+5}{10 + \\sin{(4)}}\\bigg)dx \\\\\n    &= \\frac{1}{10 + \\sin{(4)}}\\bigg[2\\int_0^2x^3\\cos{(x^2)}dx + 5\\int_0^2x^2dx\\bigg] \\\\\n    &= \\frac{x^2\\sin{(x^2)}+\\cos{(x^2)}+\\frac{5x^3}{3}}{10 + \\sin{(4)}}\\bigg{\\vert}_0^2\n\\end{align}\\]\n\nfunction stdev()\n    E2(x) = (x^2.0*sin(x^2.0) + cos(x^2.0) + (5.0x^3/3.0)) / (10.0 + sin(4.0))\n    Ex²   = E2(2.0) - E2(0.0)\n    σ² = Ex² - μ_exact^2\n    return sqrt(σ²)\nend\nσ_exact = stdev()\n\nprintln(\"The mean and the standard deviation of the PDF of X by analytical solution:\")\nprintln(\"μ = $μ_exact\")\nprintln(\"σ = $σ_exact\")\n\nThe mean and the standard deviation of the PDF of X by analytical solution:\nμ = 0.8310564083631246\nσ = 0.49541585225883805\n\n\nThe numeric and analytic solutions agree.\n\nerr_μ = (abs(μ_numeric - μ_exact) / μ_exact) * 100\nerr_σ = (abs(σ_numeric - σ_exact) / σ_exact) * 100\n\nprintln(\"Percent error in the calculated mean = $err_μ\")\nprintln(\"Percent error in the calculated standard deviation = $err_σ\")\n\nPercent error in the calculated mean = 1.3359177709872757e-14\nPercent error in the calculated standard deviation = 9.972414966246794e-13\n\n\n\n\n\n\nIn order to find the CDF numerically, we will need to write a modified version of the trapezoidal rule such that we are populating an array with the cumulative sum of every dx of the PDF of \\(X\\).\n\nfunction cumsumtrap(f::Function, x)\n    y = f.(x)\n    N = length(x)\n    dx = x[2:N] .- x[1:N-1]\n    meanY = (y[2:N] .+ y[1:N-1]) ./ 2\n    integral = cumsum(dx .* meanY)\n\n    return [0; integral]\nend\n\nfunction plotCDF()\n    fig = Figure();\n    ax = Axis(fig[1,1],\n               title = \"CDF\",\n               xlabel = \"x\",\n               ylabel = \"F(x)\"\n               )\n    r = -0.1:0.01:2.1\n    lines!(ax, r, cumsumtrap(p, r), label = \"numerical\", color = :blue)\n    xlims!(ax, -0.1, 2.1)\n    ylims!(ax, -0.1, 1.1)\n\n    display(fig)\n    return fig, ax, r\nend\nfig1, ax1, r = plotCDF();\n\n\n\n\nLet us calculate the CDF by direct integration. The CDF of a continuous random variable \\(X\\) can be expressed as the integral of its probability density function \\(p(x)\\) as:\n\\[F_X(x) = \\int_{-\\inf}^x p_X(t)dt \\tag{2}\\]\nPlugging (Equation 1) on the interval [0, 2] into (Equation 2) we get:\n\\[\\begin{align}\n    F_X(x) &= \\int_0^x\\bigg(\\frac{2t\\cos{(t^2)} + 5}{10 + \\sin{(4)}}\\bigg)dt \\\\\n    &= \\frac{1}{10 + \\sin{(4)}}\\bigg(2\\int_0^xt\\cos{(t^2)}dt + 5\\int_0^xdt\\bigg) \\\\\n    &= \\frac{\\sin{(x^2)} + 5x}{10 + \\sin{(4)}}\n\\end{align}\\]\nLet us plot the exact solution of the CDF over the numerical solution.\n\nFₓ(x) = (sin(x^2) + 5x) / (10 + sin(4))\n\nlines!(ax1, r, Fₓ.(r), label = \"exact\", color = :red, linestyle = :dash);\nfig1[1,2] = Legend(fig1, ax1, \"Legend\", framevisible = false);\ndisplay(fig1);\n\n\n\n\nThe we can plot the inverse CDF by flipping the axis.\n\nfunction plotCDFInverse()\n    fig = Figure();\n    ax = Axis(fig[1, 1],\n               title = \"Inverse CDF\",\n               xlabel = \"x\",\n               ylabel = \"F⁻¹(x)\"\n               )\n    lines!(ax, cumsumtrap(p, r), r)\n    xlims!(ax, -0.1, 1.1)\n    ylims!(ax, -0.1, 2.1)\n    display(fig);\nend\nplotCDFInverse();\n\n\n\n\n\n\n\nLet us develop a sampler for the random variable \\(X\\).\nLet the Matrix \\(\\Sigma\\) be a (N,2) matrix such that the rows are (x, y) coordinates, and the vector \\(\\textbf{x}\\) also be of size N, where each element is a sample from the Uniform normal distribution.\nBy using linear interpolation from the formula:\n\\[y = y_1 + \\frac{(x - x_1)(y_2 - y_1)}{x_2 - x_1} \\tag{3}\\]\nThe first column of \\(\\Sigma\\) comes from the cumulative trapazoidal integration of the pdf function in the range [0, 2], while the second column are equally spaced points from the range [0, 2].\n\nfunction liy(x::Float64, p1::Vector{Float64}, p2::Vector{Float64})\n    x1, y1 = p1\n    x2, y2 = p2\n    return y1 + (x - x1)*(y2 - y1)/(x2 - x1)\nend\n\nfunction sampleInverseCDF(x::Vector{Float64}, points::Matrix{Float64})\n    output = Vector{Float64}(undef, length(x))\n    for (i, x_val) in enumerate(x)\n        idx = findfirst(points[:, 1] .&gt; x_val)\n\n        if idx == nothing\n            # If x_val is greater than or equal to the last x value in inverse, use the last segment\n            p1 = points[end-1, :]\n            p2 = points[end, :]\n        elseif idx == 1\n            # If x_val is less than or equal to the first x value in inverse, use the first segment\n            p1 = points[1, :]\n            p2 = points[2, :]\n        else\n            # Otherwise, use the segment between idx-1 and idx\n            p1 = points[idx-1, :]\n            p2 = points[idx, :]\n        end\n\n        # Calculate interpolated y value using the liy function\n        output[i] = liy(x_val, p1, p2)\n    end\n    output\nend\n\nfunction plotsampledist()\n    Δr = 1e-3\n    r = -0.1:Δr:2.1\n    points = [cumsumtrap(p, r) r]\n    N = 100000\n    x = rand(N)\n    fig = Figure()\n    ax = Axis(fig[1,1], title = \"histogram of $N samples\")\n    hist!(fig[1,1], sampleInverseCDF(x, points), bins = 80, normalization = :pdf)\n    lines!(fig[1,1], r, p.(r), color = :red, label = \"p(x)\", linestyle = :dash)\n    fig[1, 2] = Legend(fig, ax, \"Legend\", framevisible = false)\n    display(fig)\nend\nplotsampledist();"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw1/hw1.html#part-a",
    "href": "courses/uncertainty_quant/docs/hw1/hw1.html#part-a",
    "title": "Homework 1",
    "section": "",
    "text": "Using the Gauss-Kronrod quadrature we can numerically solve for the mean and the variance by calculating the first moment and second momements of the PDF of X.\n\nusing QuadGK\n\nfunction E1(x::Real)\n    return x * p(x)\nend\n\nfunction E2(x::Real)\n    return x^2 * p(x)\nend\n\nfunction parta_numeric()\n    Ex  = quadgk(E1, 0.0, 2.0, rtol=1e-3)[1]\n    Ex² = quadgk(E2, 0.0, 2.0, rtol=1e-3)[1]\n    σ²  = Ex² - (Ex)^2\n    σ   = sqrt(σ²)\n    println(\"The numeric mean and standard deviation of the PDF of X are:\")\n    println(\"μ = $Ex\")\n    println(\"σ = $σ\")\n    return Ex, σ\nend\n\nμ_numeric, σ_numeric = parta_numeric();\n\nThe numeric mean and standard deviation of the PDF of X are:\nμ = 0.8310564083631247\nσ = 0.4954158522588331\n\n\nLet us solve for the first and second moments of (Equation 1) analytically and compare with the numerical findings.\n\n\n\n\\[\\begin{align}\n    E[X] &= \\int_0^2 x\\bigg(\\frac{2x\\cos{(x^2)} + 5}{10 + \\sin{(4)}}\\bigg)dx \\\\\n    &= x\\bigg(\\frac{\\sin{(x^2)} + 5x}{10 + \\sin{(4)}}\\bigg)\\bigg{\\vert}_0^2 - \\frac{1}{10 + \\sin{(4)}}\\int_0^2\\bigg(\\sin{(x^2)} + 5x\\bigg)dx \\\\\n    &= \\frac{-\\sqrt{\\frac{\\pi}{2}}S\\big(\\sqrt{\\frac{2}{\\pi}}x\\big) + \\frac{5x^2}{2} + x\\sin{(x^2)}}{10 + \\sin{(4)}}\\bigg{\\vert}_0^2 \\\\\n\\end{align}\\]\nWhere \\(S(x)\\) is the fresnel integral defined as \\[S(x) = \\int_0^x \\sin{(t^2)} dt\\]\n\nusing FresnelIntegrals\nfunction first_moment()\n    E(x) = (-sqrt(π/2.0)*fresnels(sqrt(2.0/π)*x) + (5.0/2.0)*x^2 + x*sin(x^2.0)) / (10.0 + sin(4.0))\n    μ = E(2.0) - E(0.0)\nend\nμ_exact = first_moment();\n\n\n\n\n\\[\\begin{align}\n    E[X^2] &= \\int_0^2x^2\\bigg(\\frac{2x\\cos{(x^2)}+5}{10 + \\sin{(4)}}\\bigg)dx \\\\\n    &= \\frac{1}{10 + \\sin{(4)}}\\bigg[2\\int_0^2x^3\\cos{(x^2)}dx + 5\\int_0^2x^2dx\\bigg] \\\\\n    &= \\frac{x^2\\sin{(x^2)}+\\cos{(x^2)}+\\frac{5x^3}{3}}{10 + \\sin{(4)}}\\bigg{\\vert}_0^2\n\\end{align}\\]\n\nfunction stdev()\n    E2(x) = (x^2.0*sin(x^2.0) + cos(x^2.0) + (5.0x^3/3.0)) / (10.0 + sin(4.0))\n    Ex²   = E2(2.0) - E2(0.0)\n    σ² = Ex² - μ_exact^2\n    return sqrt(σ²)\nend\nσ_exact = stdev()\n\nprintln(\"The mean and the standard deviation of the PDF of X by analytical solution:\")\nprintln(\"μ = $μ_exact\")\nprintln(\"σ = $σ_exact\")\n\nThe mean and the standard deviation of the PDF of X by analytical solution:\nμ = 0.8310564083631246\nσ = 0.49541585225883805\n\n\nThe numeric and analytic solutions agree.\n\nerr_μ = (abs(μ_numeric - μ_exact) / μ_exact) * 100\nerr_σ = (abs(σ_numeric - σ_exact) / σ_exact) * 100\n\nprintln(\"Percent error in the calculated mean = $err_μ\")\nprintln(\"Percent error in the calculated standard deviation = $err_σ\")\n\nPercent error in the calculated mean = 1.3359177709872757e-14\nPercent error in the calculated standard deviation = 9.972414966246794e-13"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw1/hw1.html#part-b",
    "href": "courses/uncertainty_quant/docs/hw1/hw1.html#part-b",
    "title": "Homework 1",
    "section": "",
    "text": "In order to find the CDF numerically, we will need to write a modified version of the trapezoidal rule such that we are populating an array with the cumulative sum of every dx of the PDF of \\(X\\).\n\nfunction cumsumtrap(f::Function, x)\n    y = f.(x)\n    N = length(x)\n    dx = x[2:N] .- x[1:N-1]\n    meanY = (y[2:N] .+ y[1:N-1]) ./ 2\n    integral = cumsum(dx .* meanY)\n\n    return [0; integral]\nend\n\nfunction plotCDF()\n    fig = Figure();\n    ax = Axis(fig[1,1],\n               title = \"CDF\",\n               xlabel = \"x\",\n               ylabel = \"F(x)\"\n               )\n    r = -0.1:0.01:2.1\n    lines!(ax, r, cumsumtrap(p, r), label = \"numerical\", color = :blue)\n    xlims!(ax, -0.1, 2.1)\n    ylims!(ax, -0.1, 1.1)\n\n    display(fig)\n    return fig, ax, r\nend\nfig1, ax1, r = plotCDF();\n\n\n\n\nLet us calculate the CDF by direct integration. The CDF of a continuous random variable \\(X\\) can be expressed as the integral of its probability density function \\(p(x)\\) as:\n\\[F_X(x) = \\int_{-\\inf}^x p_X(t)dt \\tag{2}\\]\nPlugging (Equation 1) on the interval [0, 2] into (Equation 2) we get:\n\\[\\begin{align}\n    F_X(x) &= \\int_0^x\\bigg(\\frac{2t\\cos{(t^2)} + 5}{10 + \\sin{(4)}}\\bigg)dt \\\\\n    &= \\frac{1}{10 + \\sin{(4)}}\\bigg(2\\int_0^xt\\cos{(t^2)}dt + 5\\int_0^xdt\\bigg) \\\\\n    &= \\frac{\\sin{(x^2)} + 5x}{10 + \\sin{(4)}}\n\\end{align}\\]\nLet us plot the exact solution of the CDF over the numerical solution.\n\nFₓ(x) = (sin(x^2) + 5x) / (10 + sin(4))\n\nlines!(ax1, r, Fₓ.(r), label = \"exact\", color = :red, linestyle = :dash);\nfig1[1,2] = Legend(fig1, ax1, \"Legend\", framevisible = false);\ndisplay(fig1);\n\n\n\n\nThe we can plot the inverse CDF by flipping the axis.\n\nfunction plotCDFInverse()\n    fig = Figure();\n    ax = Axis(fig[1, 1],\n               title = \"Inverse CDF\",\n               xlabel = \"x\",\n               ylabel = \"F⁻¹(x)\"\n               )\n    lines!(ax, cumsumtrap(p, r), r)\n    xlims!(ax, -0.1, 1.1)\n    ylims!(ax, -0.1, 2.1)\n    display(fig);\nend\nplotCDFInverse();"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw1/hw1.html#part-c",
    "href": "courses/uncertainty_quant/docs/hw1/hw1.html#part-c",
    "title": "Homework 1",
    "section": "",
    "text": "Let us develop a sampler for the random variable \\(X\\).\nLet the Matrix \\(\\Sigma\\) be a (N,2) matrix such that the rows are (x, y) coordinates, and the vector \\(\\textbf{x}\\) also be of size N, where each element is a sample from the Uniform normal distribution.\nBy using linear interpolation from the formula:\n\\[y = y_1 + \\frac{(x - x_1)(y_2 - y_1)}{x_2 - x_1} \\tag{3}\\]\nThe first column of \\(\\Sigma\\) comes from the cumulative trapazoidal integration of the pdf function in the range [0, 2], while the second column are equally spaced points from the range [0, 2].\n\nfunction liy(x::Float64, p1::Vector{Float64}, p2::Vector{Float64})\n    x1, y1 = p1\n    x2, y2 = p2\n    return y1 + (x - x1)*(y2 - y1)/(x2 - x1)\nend\n\nfunction sampleInverseCDF(x::Vector{Float64}, points::Matrix{Float64})\n    output = Vector{Float64}(undef, length(x))\n    for (i, x_val) in enumerate(x)\n        idx = findfirst(points[:, 1] .&gt; x_val)\n\n        if idx == nothing\n            # If x_val is greater than or equal to the last x value in inverse, use the last segment\n            p1 = points[end-1, :]\n            p2 = points[end, :]\n        elseif idx == 1\n            # If x_val is less than or equal to the first x value in inverse, use the first segment\n            p1 = points[1, :]\n            p2 = points[2, :]\n        else\n            # Otherwise, use the segment between idx-1 and idx\n            p1 = points[idx-1, :]\n            p2 = points[idx, :]\n        end\n\n        # Calculate interpolated y value using the liy function\n        output[i] = liy(x_val, p1, p2)\n    end\n    output\nend\n\nfunction plotsampledist()\n    Δr = 1e-3\n    r = -0.1:Δr:2.1\n    points = [cumsumtrap(p, r) r]\n    N = 100000\n    x = rand(N)\n    fig = Figure()\n    ax = Axis(fig[1,1], title = \"histogram of $N samples\")\n    hist!(fig[1,1], sampleInverseCDF(x, points), bins = 80, normalization = :pdf)\n    lines!(fig[1,1], r, p.(r), color = :red, label = \"p(x)\", linestyle = :dash)\n    fig[1, 2] = Legend(fig, ax, \"Legend\", framevisible = false)\n    display(fig)\nend\nplotsampledist();"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Consider the random variable: \\[\\begin{align}\nY = \\sum_{j = 1}^{n}b_jX_j\n\\end{align}\\]\nwhere at least one \\(b_j\\) is non-zero, and \\((X_1, ..., X_n)\\) are jointly Gaussian random variables with mean \\(\\mathbf{\\mu} = (\\mu_1, ..., \\mu_{n})\\) and convergence matrix \\(\\mathbf{\\Sigma}\\).\n\n\n\n\n\nThe characteristic function of a jointly Gaussian random variable:\n\n\\[\\begin{align}\n\\phi_X(t) = e^{it^T\\mu - \\frac{1}{2} t^T\\Sigma t}\n\\end{align}\\]\n\nLet \\(\\mathbf{B} = \\left[b_1, b_2, ..., b_n\\right]^T\\) such that \\(Y = \\mathbf{B^T X} = b_1x_1 + b_2x_2 + ... + b_nx_n\\).\nLet us find the characteristic function of \\(Y\\):\n\n\\[\\begin{align}\n    \\phi_Y(t) &= \\mathbb{E}\\left[e^{it \\sum_{j=1}^{n}b_j X_j}\\right]\\\\\n    &= \\phi_X(tb_1, tb_2, ..., tb_n) \\\\\n    &= e^{it\\left(B^T \\mu\\right) - \\frac{1}{2} t^2 \\left(B^T \\Sigma B\\right)}\n\\end{align}\\]\n\nThus \\(Y\\) is a normally distributed Gaussian random variable via its characteristic function.\n\n\n\n\n\n\nFrom the characteristic function in part A, we can see that the mean and variance of \\(Y\\) is given as:\n\\[\\begin{align}\n    \\mathbb{E}\\left[Y\\right] &= B^T \\mu \\\\\n    \\ \\\\\n    \\mathbb{E}\\left[Y^2\\right] - \\mathbb{E}\\left[Y\\right]^2 &= B^T \\Sigma B\n\\end{align}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#problem-definition",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#problem-definition",
    "title": "Homework 2",
    "section": "",
    "text": "Consider the random variable: \\[\\begin{align}\nY = \\sum_{j = 1}^{n}b_jX_j\n\\end{align}\\]\nwhere at least one \\(b_j\\) is non-zero, and \\((X_1, ..., X_n)\\) are jointly Gaussian random variables with mean \\(\\mathbf{\\mu} = (\\mu_1, ..., \\mu_{n})\\) and convergence matrix \\(\\mathbf{\\Sigma}\\)."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#part-a",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#part-a",
    "title": "Homework 2",
    "section": "",
    "text": "The characteristic function of a jointly Gaussian random variable:\n\n\\[\\begin{align}\n\\phi_X(t) = e^{it^T\\mu - \\frac{1}{2} t^T\\Sigma t}\n\\end{align}\\]\n\nLet \\(\\mathbf{B} = \\left[b_1, b_2, ..., b_n\\right]^T\\) such that \\(Y = \\mathbf{B^T X} = b_1x_1 + b_2x_2 + ... + b_nx_n\\).\nLet us find the characteristic function of \\(Y\\):\n\n\\[\\begin{align}\n    \\phi_Y(t) &= \\mathbb{E}\\left[e^{it \\sum_{j=1}^{n}b_j X_j}\\right]\\\\\n    &= \\phi_X(tb_1, tb_2, ..., tb_n) \\\\\n    &= e^{it\\left(B^T \\mu\\right) - \\frac{1}{2} t^2 \\left(B^T \\Sigma B\\right)}\n\\end{align}\\]\n\nThus \\(Y\\) is a normally distributed Gaussian random variable via its characteristic function."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#part-b",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#part-b",
    "title": "Homework 2",
    "section": "",
    "text": "From the characteristic function in part A, we can see that the mean and variance of \\(Y\\) is given as:\n\\[\\begin{align}\n    \\mathbb{E}\\left[Y\\right] &= B^T \\mu \\\\\n    \\ \\\\\n    \\mathbb{E}\\left[Y^2\\right] - \\mathbb{E}\\left[Y\\right]^2 &= B^T \\Sigma B\n\\end{align}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#problem-definition-1",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#problem-definition-1",
    "title": "Homework 2",
    "section": "Problem definition",
    "text": "Problem definition\nConsider the two-dimensional random vector \\(\\mathbf{X} = \\left[X_1, X_2\\right]\\) with joint PDF\n\\[  p(x_1, x_2) =\n    \\begin{cases}\n        K\\cos^2{(10x_1x_2)} & (x_1, x_2) \\in \\left[0, 1\\right] \\times \\left[0,1\\right] \\\\\n        0 & \\text{otherwise}\n    \\end{cases} \\tag{1}\\]\nwhere\n\\[\\begin{align}\n    K = \\frac{40}{Si(20) + 20} \\quad , \\quad Si(x) = \\int_{0}^{x} \\frac{\\sin{(t)}}{t} dt\n\\end{align}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#part-a-1",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#part-a-1",
    "title": "Homework 2",
    "section": "Part A",
    "text": "Part A\n\nCompute the conditional PDFs \\(p(x_1 | x_2)\\) and \\(p(x_2 | x_1)\\).\nLet us compute the conditional probability of \\(p(x_1|x_2)\\)\n\\[\\begin{align}\n    p(x_1 | x_2) &= \\frac{p(x_1, x_2)}{p(x_2)} \\\\\n    &= \\frac{p(x_1, x_2)}{\\int_{-\\infty}^{\\infty} p(x_1, x_2) dx_1} \\\\\n    &= \\frac{K\\cos^2{(10x_1x_2)}}{K\\int_{0}^{1} \\cos^2{(10x_1x_2)} dx_1} \\\\\n    &= \\frac{K\\cos^2{(10x_1x_2)}}{\\frac{K}{10x_2}\\int_{0}^{10x_2}\\cos^2{(u)}du} \\\\\n    &= \\frac{K\\cos^2{(10x_1x_2)}}{\\frac{K}{10x_2}\\left[\\frac{1}{2}\\left(u + \\sin{(u)}\\cos{(u)}\\right)\\right]_{0}^{10x_2}}\\\\\n    &= \\frac{K\\cos^2{(10x_1x_2)}}{\\frac{K}{40x_2}\\left(2u + \\sin{(2u)}\\right)\\bigg{|}_{0}^{10x_2}} \\\\\n    &= \\frac{40x_2\\cos^2{(10x_1x_2)}}{20x_2 + \\sin{(20x_2)}}\n\\end{align}\\]\nBy taking the integral with respect to \\(x_2\\) we have that \\[p(x_1) = K\\left(\\frac{\\sin{(20x_1)}}{40x_1} + \\frac{1}{2}\\right)\\]\nthus the conditional probabilities are: \\[\\begin{align}\n    p(x_1 | x_2) &= \\frac{40x_2\\cos^2{(10x_1x_2)}}{20x_2 + \\sin{(20x_2)}} \\\\\n    \\ \\\\\n    p(x_2 | x_1) &= \\frac{40x_1\\cos^2{(10x_1x_2)}}{20x_1 + \\sin{(20x_1)}}\n\\end{align}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#part-b-1",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#part-b-1",
    "title": "Homework 2",
    "section": "Part B",
    "text": "Part B\n\nWrite a computer code that samples the PDF of (Equation 1) using the gibbs sampling algorithm. Plot the PDF and the samples you obtain from the Gibbs algorithm on a 2d contour plot, similarly to Figure 3 in the course note 2.\n\nusing GLMakie\nusing QuadGK\n\n# cumulative trapazoidal rule\nfunction cumsumtrap(f::Function, x)\n    y = f.(x)\n    N = length(x)\n    x1 = @view x[1:N-1]\n    x2 = @view x[2:N]\n    y1 = @view y[1:N-1]\n    y2 = @view y[2:N]\n    integral = cumsum(((x2.-x1).*(y1.+y2))./2.0)\n    integral ./= integral[end]\n    return [0; integral]\nend\n\n# CDF inverse sampler\nfunction sampleInverseCDF(x::Float64, points::Matrix{Float64})\n    idx = findfirst(points[:, 1] .&gt; x)\n    if idx === nothing\n        p1 = points[end-1, :]\n        p2 = points[end, :]\n    elseif idx == 1\n        p1 = points[1, :]\n        p2 = points[2, :]\n    else\n        p1 = points[idx-1, :]\n        p2 = points[idx, :]\n    end\n    liy(x, p1, p2)\nend\n\n# Linear Interpolator\nfunction liy(x::Float64, p1::Vector{Float64}, p2::Vector{Float64})\n    x1, y1 = p1\n    x2, y2 = p2\n    if isapprox(x1, x2, atol = 1e-12)\n        return (y1 + y2) / 2.0\n    end\n    return y1 + (x - x1)*(y2 - y1)/(x2 - x1)\nend\n\n# Sine Integral\nfunction Si(x::Float64)\n    return x == 0.0 ? 0.0 : quadgk(t -&gt; sin(t)/t, 0.0, x, rtol=1e-3)[1]\nend\n\n# joint PDF for problem 2 p(x, y)\nfunction p(x::Float64, y::Float64)\n    if x &lt; 0.0 || y &lt; 0.0\n        return 0.0\n    end\n    return ((40.0)/(Si(20.0) + 20.0))*cos(10.0*x*y)*cos(10.0*x*y)\nend\n\n# Conditional PDF p(x | y)\nfunction pxGy(x::Float64, y::Float64)\n    denom = (20.0*y+sin(20.0*y))\n    if abs(denom) &lt; 1e-6\n        return 0.0\n    end\n    return (40.0*y*cos(10.0*x*y)*cos(10.0*x*y))/denom\nend\n\n# Conditional PDF p(y | x)\nfunction pyGx(x::Float64, y::Float64)\n    denom = (20.0*x+sin(20.0*x))\n    if abs(denom) &lt; 1e-6\n        return 0.0\n    end\n    return (40.0*x*cos(10.0*x*y)*cos(10.0*x*y))/denom\nend\n\n# Mesh of the surface of the joint PDF\nfunction getSurface()\n    xs = LinRange(0, 1, 100)\n    ys = LinRange(0, 1, 100)\n    zs = [p(x, y) for x in xs, y in ys]\n    return xs, ys, zs\nend\n\n# gibbs sampler\nfunction gibbsSample(N::Integer)\n    samples = Matrix{Float64}(undef, N, 2)\n    r = LinRange(0.0, 1.0, 1000)\n    # initialize random x1\n    y0 = rand()\n    samples[1, 1] = sampleInverseCDF(rand(), hcat(cumsumtrap(x -&gt; pxGy(x, y0), r), r))\n    for i=2:N\n        samples[i-1, 2] = sampleInverseCDF(rand(), hcat(cumsumtrap(y -&gt; pyGx(samples[i-1, 1], y), r), r))\n        samples[i, 1] = sampleInverseCDF(rand(), hcat(cumsumtrap(x -&gt; pxGy(x, samples[i-1, 2]), r), r))\n    end\n    samples\nend\n\n# Part B problem solution\nfunction partb(N::Integer)\n    fig = Figure()\n    ax = Axis(fig[1,1])\n    xs, ys, zs = getSurface()\n    co = contourf!(ax, xs, ys, zs,\n                   extendlow = :auto,\n                   extendhigh = :auto)\n    samples = gibbsSample(N)\n    scatter!(ax, samples[:, 1], samples[:, 2], markersize = 3, color = :red)\n    Colorbar(fig[1, 2], co)\n    save(\"q2partb.png\", fig)\nend\npartb(5000);\n\n\n\n\nPlot of 5000 samples from the joint PDF using Gibbs sampling"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#part-c",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#part-c",
    "title": "Homework 2",
    "section": "Part C",
    "text": "Part C\nWrite a program to calculate the sample mean and sample standard deviation of the random function\n\\[f(y;X_1, X_2) = \\sin{(4 \\pi X_1 y)} + \\cos{(4 \\pi X_2 y)} \\quad y \\in [0,1] \\tag{2}\\]\nwhere \\(X_1\\) and \\(X_2\\) are random variables with joint PDF given by (Equation 1).\n\nfunction f(y::Float64, X1::Float64, X2::Float64)\n    return sin(4*π*X1*y) + cos(4*π*X2*y)\nend\n\nfunction partc()\n    N = Int64(5e4)\n    M = Int64(500)\n    r = LinRange(0.0, 1.0, M)\n    fig = Figure()\n    grid = fig[1, 1] = GridLayout()\n    ax = Axis(grid[1, 1],\n              title = \"y by sample mean and standard deviation\",\n              xlabel = \"y\")\n    samples = Matrix{Float64}(undef, N, M)\n    for i=1:N\n        X1, X2 = gibbsSample(2)[:, 1]\n        for j=1:M\n            samples[i, j] = f(r[j], X1, X2)\n        end\n    end\n    μᵢ = vec(sum(j -&gt; j, samples, dims=1) ./ N)\n    μₜ = sum(μᵢ) / M\n    σᵢ = vec(sqrt.(sum((samples .- μᵢ') .^ 2, dims = 1) ./ (N - 1)))\n    means = lines!(ax, r, μᵢ, color = :red)\n    stdv = lines!(ax, r, σᵢ, color = :blue)\n    Legend(fig[1, 2], [means, stdv], [\"mean\", \"std dev\"])\n    save(\"q2partc.png\", fig)\nend\npartc();\n\n\n\n\nPlot of the mean and standard deviation for 50000 samples on a 500 spatial grid of evenly distributed values of y from 0 to 1"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#problem-definition-2",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#problem-definition-2",
    "title": "Homework 2",
    "section": "Problem definition",
    "text": "Problem definition\n\nWrite a computer code that estimates the PDF of the random variable\n\\[\\bar{X}_N = \\frac{X_1 + ... + X_N}{N} \\quad \\text{sample mean} \\tag{3}\\]\nwhere \\(\\{X_j\\}\\) are independent identically distributed Cauchy random variables with PDF\n\\[\\begin{equation}\n    p_{X_j}(x) = \\frac{1}{\\pi (1 + x^2)} \\quad j = 1, ..., N\n\\end{equation}\\]\nusing the inverse CDF function and relative frequency approach you developed in HW1.\nPlot your results for \\(N = 10^3\\) and \\(N = 10^5\\)."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw2/hw2.html#solution",
    "href": "courses/uncertainty_quant/docs/hw2/hw2.html#solution",
    "title": "Homework 2",
    "section": "Solution",
    "text": "Solution\nLet us find the inverse cdf of the cauchy distribution:\n\\[\\begin{align}\n    F(x) = \\frac{1}{\\pi} \\int_{-\\infty}{x}\\frac{1}{(1 + y^2)}dy &= \\frac{1}{\\pi}\\left[\\arctan{(y)}\\bigg{|}_{-\\infty}^x\\right] \\\\\n    &= \\frac{1}{\\pi}\\left[\\arctan{(x)} - \\arctan{(-\\infty)}\\right] \\\\\n\\end{align}\\]\n\\[\\begin{align}\n    F^{-1}(x) = \\tan{(\\pi(x - \\frac{1}{2}))}\n\\end{align}\\]\n\nusing KernelDensity\nfunction PDF(x::Float64)\n    return 1.0 / (π*(1+x^2))\nend\n\nfunction inverseCDF(x::Float64)\n    return tan(π*(x - 0.5))\nend\n\nfunction question3(N::Integer)\n    samples = Vector{Float64}(undef, 1000)\n    for j in eachindex(samples)\n        s = Vector{Float64}(undef, N)\n        for i in eachindex(s)\n            s[i] = inverseCDF(rand())\n        end\n        samples[j] = sum(s) / Float64(lastindex(s))\n    end\n\n    fig = Figure()\n    ax = Axis(fig[1, 1])\n    k = kde(samples)\n    r = LinRange(-10.0, 10.0, 1000)\n    lines!(ax, r, PDF.(r), color = :orange, label = \"PDF\")\n    lines!(ax, k.x, k.density, color = :red, label = \"KDE N=$N\", linestyle = :dash)\n    xlims!(ax, -10, 10)\n    fig[1, 2] = Legend(fig, ax, \"Legend\", framevisible = false)\n    save(\"question3_$(N).png\", fig)\nend\nquestion3(1000);\nquestion3(100000);\n\n\n\n\nKernel Density estimate of the PDF of the random variable \\(\\bar{X}_N\\) from 1000 samples with N = 1000\n\n\n\n\n\nKernel Density estimate of the PDF of the random variable \\(\\bar{X}_N\\) from 1000 samples with N = 100000"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Consider a random variable \\(\\xi\\) with PDF\n\\[p_{\\xi}(x) =\n\\begin{cases}\n\\frac{e^{-x}}{e - e^{-1}} & x \\in [-1, 1] \\\\\n0 & \\text{otherwise}\n\\end{cases} \\tag{1}\\]\n\n\nUse the Stieltjes algorithm to compute the sixth-order generalized polynomial choas basis \\[\\{P_0(x), P_1(x), ..., P_6(x)\\}\\] for \\(\\xi\\), i.e. a set of polynomials up to degree 6 that are orthogonal relative to the PDF \\(\\xi\\) given in Equation 1.\n\n\nWe know that since the distribution function Equation 1 is compactly supported, that the solution to the moment problem is unique and exists.\nLet \\(c = \\frac{1}{e-e^{-1}}\\) the constant in \\(p_{\\xi}(x)\\)\nFollowing the Stieltjes algorithm, let us compute the first orthogonal polynomial and then we will write a code that computes the first six orthogonal polynomials where \\[\\begin{align}\n\\mu(x) = ce^{-x}\n\\end{align}\\]\nis out weight function.\nLet \\(n = 0 \\quad \\pi_0 = 1 \\quad \\pi_{-1} = 0\\)\n\\[\\begin{align}\n\\alpha_0 &= \\frac{\\langle x , 1 \\rangle}{\\langle 1, 1 \\rangle} \\\\\n&= \\frac{c \\int_{-1}^{1}x e^{-x}dx}{c \\int_{-1}^{1}e^{-x}dx} \\\\\n&= \\frac{\\left[{-x e^{-x}}_{\\bigg{\\vert}_{-1}^{1}} - \\int_{-1}^{1}-e^{-x}dx\\right]}{\\left[{-e^{-x}}_{\\bigg{\\vert}_{-1}^{1}}\\right] } \\\\\n&= \\frac{-e^{-1} - e - e^{-1} + e}{-e^{-1} + e} \\\\\n&= -\\frac{2}{e^2 - 1} \\approx -0.31304\n\\end{align}\\]\nUsing \\(\\alpha_0\\) let us find the first polynomial \\(\\pi_1\\) using the following formula\n\\[\\begin{align}\n\\pi_{n+1}(x) = (x - \\alpha_n)\\pi_n(x) - \\beta_n \\pi_{n-1}(x)\n\\end{align}\\]\n\\[\\begin{align}\n\\pi_1(x) &= (x - \\alpha_0)\\pi_0 - \\beta_0 \\pi_{-1} \\\\\n&= x + \\frac{2}{e^2 - 1}\n\\end{align}\\]\nLet us now write a code that produces any arbitray number of orthogonal polynomials with respect to the weight function.\n\nusing GLMakie\nusing QuadGK\nusing StaticArrays\nusing Statistics\n\n# weight function\nμ(x) = exp(-x) * ^((exp(1.0) - ^(exp(1.0), -1.0)), -1.0)\n\n# define an integral using gauss-kronrod quadrature rule\ninteg(x::Function, sup::SVector{2}) = quadgk(x, sup[1], sup[2]; atol=1e-8, rtol=1e-8)[1]\n\n# the support of the weight function\nsup = SVector{2}(-1.0, 1.0)\n\n# cumulative trapazoidal rule\nfunction cumsumtrap(f::Function, x)\n    y = f.(x)\n    N = length(x)\n    x1 = @view x[1:N-1]\n    x2 = @view x[2:N]\n    y1 = @view y[1:N-1]\n    y2 = @view y[2:N]\n    integral = cumsum(((x2.-x1).*(y1.+y2))./2.0)\n    integral ./= integral[end]\n    return [0; integral]\nend\n\n# CDF inverse sampler\nfunction sampleInverseCDF(x::Float64, points::Matrix{Float64})\n    idx = findfirst(points[:, 1] .&gt; x)\n    if idx === nothing\n        p1 = points[end-1, :]\n        p2 = points[end, :]\n    elseif idx == 1\n        p1 = points[1, :]\n        p2 = points[2, :]\n    else\n        p1 = points[idx-1, :]\n        p2 = points[idx, :]\n    end\n    liy(x, p1, p2)\nend\n\n# Linear Interpolator\nfunction liy(x::Float64, p1::Vector{Float64}, p2::Vector{Float64})\n    x1, y1 = p1\n    x2, y2 = p2\n    if isapprox(x1, x2, atol = 1e-12)\n        return (y1 + y2) / 2.0\n    end\n    return y1 + (x - x1)*(y2 - y1)/(x2 - x1)\nend\n\n# stieltjes algorithm\nfunction stieltjes(μ::Function, N::Int64, sup::SVector{2})\n    # μ: weight function defining the inner product\n    # N: number of orthogonal polynomials to compute\n    # sup: support (integration bounds) of the weight function\n\n    M = N + 2  # Extend size to accommodate buffer\n    n = 2      # Starting index for the recursion\n\n    # Initialize orthogonal polynomials (πn) as functions\n    π = Vector{Function}(undef, M)\n    π[n-1] = x -&gt; 0.0 * x^0.0  # π₀(x) = 0\n    π[n] = x -&gt; 1.0 * x^0.0    # π₁(x) = 1\n\n    # Initialize coefficient vectors αn and βn\n    α = Vector{Float64}(undef, M)\n    β = Vector{Float64}(undef, M)\n    β[n-1] = 0.0  # β₀ = 0\n    β[n] = 0.0    # β₁ = 0\n\n    # Compute the first α coefficient (α₂)\n    # α₂ = ⟨xπ₁, π₁⟩ / ⟨π₁, π₁⟩\n    α[n] = integ(x -&gt; x * π[n](x) * π[n](x) * μ(x), sup) / integ(x -&gt; π[n](x) * π[n](x) * μ(x), sup)\n\n    # Compute the next orthogonal polynomial π₂\n    # π₂(x) = (x - α₁)π₁(x) - β₁π₀(x)\n    π[n+1] = x -&gt; (x - α[n]) * π[n](x) - β[n] * π[n-1](x)\n\n    for n in 3:M-1\n        α[n] = integ(x -&gt; x * π[n](x) * π[n](x) * μ(x), sup) / integ(x -&gt; π[n](x) * π[n](x) * μ(x), sup)\n        β[n] = integ(x -&gt; π[n](x) * π[n](x) * μ(x), sup) / integ(x -&gt; π[n-1](x) * π[n-1](x) * μ(x), sup)\n        π[n+1] = π[n+1] = x -&gt; (x - α[n]) * π[n](x) - β[n] * π[n-1](x)\n    end\n    return π\nend\nπ = stieltjes(μ, 6, sup)\n\n8-element Vector{Function}:\n #1 (generic function with 1 method)\n #2 (generic function with 1 method)\n #5 (generic function with 1 method)\n #10 (generic function with 1 method)\n #10 (generic function with 1 method)\n #10 (generic function with 1 method)\n #10 (generic function with 1 method)\n #10 (generic function with 1 method)\n\n\nLet us define a function that plots the first polynomial \\(\\pi_1(x)\\) over the numerical result as a validation.\n\nfunction validatepione(π::Vector{Function})\n    π_1(x) = x + (2 / (exp(1)^2 - 1))\n    xs = LinRange(-1, 1, 1000)\n    fig = Figure()\n    ax = Axis(fig[1, 1], title = \"first orthogonal polynomial validation\")\n    lines!(ax, xs, π_1.(xs), label = \"analytical\")\n    lines!(ax, xs, π[3].(xs), label = \"numerical\", linestyle = :dash, color = :red)\n    Legend(fig[1, 2], ax)\n    save(\"validationpione.png\", fig)\nend\nvalidatepione(π);\n\n\n\n\nValidation of the stieltjes algorthim: Plot of numerical over analytical solution of the first orthogonal polynomial\n\n\n\n\n\n\nVeriy that the polynomial basis you obtained in part a is orthogonal, i.e., that the matrix\n\\[\\mathbb{E}\\{P_k(\\xi)P_j(\\xi)\\} = \\int_{-1}^{1} P_k(x)P_j(x)dx \\tag{2}\\]\nis diagonal.\n\n\nLet us write a code that computes the Matrix and then plot the matrix using a heatmap to check if it diagonal.\n\nfunction isdiagonal(π::Vector{Function}, sup::SVector{2})\n    A = Matrix{Float64}(undef, 7, 7)\n    for idx in CartesianIndices(A)\n        (k, j) = idx.I\n        ele = integ(x-&gt; π[k+1](x) * π[j+1](x) * μ(x), sup)\n        A[k, j] = ele &lt; 1e-12 ? 1e-8 : ele\n    end\n    fig = Figure()\n    ax = Axis(fig[1, 1], title = \"heatmap of the diagona matrix\")\n    heatmap!(ax, log10.(A))\n    ax.yreversed=true\n    save(\"heatmapdiagonal.png\", fig)\nend\nisdiagonal(π, sup)\n\n\n\n\nHeatmap showing that the matrix of innerproducts with respect to the weight function of all polynomials generated is diagonal.\n\n\nClearly we can see that the matrix is diagonal and thus the polynomial functions are orthogonal with respect to the weight function.\n\n\n\n\nPlot \\(P_k(x)\\) for \\(k = 0, ..., 6\\)\n\n\nLet us define a code that plots all polynomials \\(P_k(x)\\) for \\(k = 0, ..., 6\\)\n\nfunction plotpolynomials(π::Vector{Function})\n    M = size(π)[1]\n    fig = Figure();display(fig)\n    ax = Axis(fig[1, 1], title = \"Plot of the set of orthogonal polynomials up to degree 6\")\n    xs = LinRange(-1.0, 1.0, 1000)\n    for n in 1:M-1\n        lines!(ax, xs, π[n+1].(xs), label=\"π_$(n-1)\")\n    end\n    Legend(fig[1, 2], ax)\n    save(\"plotpolynomials.png\", fig)\nend\nplotpolynomials(π)\n\n\n\n\nPlot of polynomials within the support [-1, 1]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#part-a",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#part-a",
    "title": "Homework 5",
    "section": "",
    "text": "Use the Stieltjes algorithm to compute the sixth-order generalized polynomial choas basis \\[\\{P_0(x), P_1(x), ..., P_6(x)\\}\\] for \\(\\xi\\), i.e. a set of polynomials up to degree 6 that are orthogonal relative to the PDF \\(\\xi\\) given in Equation 1.\n\n\nWe know that since the distribution function Equation 1 is compactly supported, that the solution to the moment problem is unique and exists.\nLet \\(c = \\frac{1}{e-e^{-1}}\\) the constant in \\(p_{\\xi}(x)\\)\nFollowing the Stieltjes algorithm, let us compute the first orthogonal polynomial and then we will write a code that computes the first six orthogonal polynomials where \\[\\begin{align}\n\\mu(x) = ce^{-x}\n\\end{align}\\]\nis out weight function.\nLet \\(n = 0 \\quad \\pi_0 = 1 \\quad \\pi_{-1} = 0\\)\n\\[\\begin{align}\n\\alpha_0 &= \\frac{\\langle x , 1 \\rangle}{\\langle 1, 1 \\rangle} \\\\\n&= \\frac{c \\int_{-1}^{1}x e^{-x}dx}{c \\int_{-1}^{1}e^{-x}dx} \\\\\n&= \\frac{\\left[{-x e^{-x}}_{\\bigg{\\vert}_{-1}^{1}} - \\int_{-1}^{1}-e^{-x}dx\\right]}{\\left[{-e^{-x}}_{\\bigg{\\vert}_{-1}^{1}}\\right] } \\\\\n&= \\frac{-e^{-1} - e - e^{-1} + e}{-e^{-1} + e} \\\\\n&= -\\frac{2}{e^2 - 1} \\approx -0.31304\n\\end{align}\\]\nUsing \\(\\alpha_0\\) let us find the first polynomial \\(\\pi_1\\) using the following formula\n\\[\\begin{align}\n\\pi_{n+1}(x) = (x - \\alpha_n)\\pi_n(x) - \\beta_n \\pi_{n-1}(x)\n\\end{align}\\]\n\\[\\begin{align}\n\\pi_1(x) &= (x - \\alpha_0)\\pi_0 - \\beta_0 \\pi_{-1} \\\\\n&= x + \\frac{2}{e^2 - 1}\n\\end{align}\\]\nLet us now write a code that produces any arbitray number of orthogonal polynomials with respect to the weight function.\n\nusing GLMakie\nusing QuadGK\nusing StaticArrays\nusing Statistics\n\n# weight function\nμ(x) = exp(-x) * ^((exp(1.0) - ^(exp(1.0), -1.0)), -1.0)\n\n# define an integral using gauss-kronrod quadrature rule\ninteg(x::Function, sup::SVector{2}) = quadgk(x, sup[1], sup[2]; atol=1e-8, rtol=1e-8)[1]\n\n# the support of the weight function\nsup = SVector{2}(-1.0, 1.0)\n\n# cumulative trapazoidal rule\nfunction cumsumtrap(f::Function, x)\n    y = f.(x)\n    N = length(x)\n    x1 = @view x[1:N-1]\n    x2 = @view x[2:N]\n    y1 = @view y[1:N-1]\n    y2 = @view y[2:N]\n    integral = cumsum(((x2.-x1).*(y1.+y2))./2.0)\n    integral ./= integral[end]\n    return [0; integral]\nend\n\n# CDF inverse sampler\nfunction sampleInverseCDF(x::Float64, points::Matrix{Float64})\n    idx = findfirst(points[:, 1] .&gt; x)\n    if idx === nothing\n        p1 = points[end-1, :]\n        p2 = points[end, :]\n    elseif idx == 1\n        p1 = points[1, :]\n        p2 = points[2, :]\n    else\n        p1 = points[idx-1, :]\n        p2 = points[idx, :]\n    end\n    liy(x, p1, p2)\nend\n\n# Linear Interpolator\nfunction liy(x::Float64, p1::Vector{Float64}, p2::Vector{Float64})\n    x1, y1 = p1\n    x2, y2 = p2\n    if isapprox(x1, x2, atol = 1e-12)\n        return (y1 + y2) / 2.0\n    end\n    return y1 + (x - x1)*(y2 - y1)/(x2 - x1)\nend\n\n# stieltjes algorithm\nfunction stieltjes(μ::Function, N::Int64, sup::SVector{2})\n    # μ: weight function defining the inner product\n    # N: number of orthogonal polynomials to compute\n    # sup: support (integration bounds) of the weight function\n\n    M = N + 2  # Extend size to accommodate buffer\n    n = 2      # Starting index for the recursion\n\n    # Initialize orthogonal polynomials (πn) as functions\n    π = Vector{Function}(undef, M)\n    π[n-1] = x -&gt; 0.0 * x^0.0  # π₀(x) = 0\n    π[n] = x -&gt; 1.0 * x^0.0    # π₁(x) = 1\n\n    # Initialize coefficient vectors αn and βn\n    α = Vector{Float64}(undef, M)\n    β = Vector{Float64}(undef, M)\n    β[n-1] = 0.0  # β₀ = 0\n    β[n] = 0.0    # β₁ = 0\n\n    # Compute the first α coefficient (α₂)\n    # α₂ = ⟨xπ₁, π₁⟩ / ⟨π₁, π₁⟩\n    α[n] = integ(x -&gt; x * π[n](x) * π[n](x) * μ(x), sup) / integ(x -&gt; π[n](x) * π[n](x) * μ(x), sup)\n\n    # Compute the next orthogonal polynomial π₂\n    # π₂(x) = (x - α₁)π₁(x) - β₁π₀(x)\n    π[n+1] = x -&gt; (x - α[n]) * π[n](x) - β[n] * π[n-1](x)\n\n    for n in 3:M-1\n        α[n] = integ(x -&gt; x * π[n](x) * π[n](x) * μ(x), sup) / integ(x -&gt; π[n](x) * π[n](x) * μ(x), sup)\n        β[n] = integ(x -&gt; π[n](x) * π[n](x) * μ(x), sup) / integ(x -&gt; π[n-1](x) * π[n-1](x) * μ(x), sup)\n        π[n+1] = π[n+1] = x -&gt; (x - α[n]) * π[n](x) - β[n] * π[n-1](x)\n    end\n    return π\nend\nπ = stieltjes(μ, 6, sup)\n\n8-element Vector{Function}:\n #1 (generic function with 1 method)\n #2 (generic function with 1 method)\n #5 (generic function with 1 method)\n #10 (generic function with 1 method)\n #10 (generic function with 1 method)\n #10 (generic function with 1 method)\n #10 (generic function with 1 method)\n #10 (generic function with 1 method)\n\n\nLet us define a function that plots the first polynomial \\(\\pi_1(x)\\) over the numerical result as a validation.\n\nfunction validatepione(π::Vector{Function})\n    π_1(x) = x + (2 / (exp(1)^2 - 1))\n    xs = LinRange(-1, 1, 1000)\n    fig = Figure()\n    ax = Axis(fig[1, 1], title = \"first orthogonal polynomial validation\")\n    lines!(ax, xs, π_1.(xs), label = \"analytical\")\n    lines!(ax, xs, π[3].(xs), label = \"numerical\", linestyle = :dash, color = :red)\n    Legend(fig[1, 2], ax)\n    save(\"validationpione.png\", fig)\nend\nvalidatepione(π);\n\n\n\n\nValidation of the stieltjes algorthim: Plot of numerical over analytical solution of the first orthogonal polynomial"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#part-b",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#part-b",
    "title": "Homework 5",
    "section": "",
    "text": "Veriy that the polynomial basis you obtained in part a is orthogonal, i.e., that the matrix\n\\[\\mathbb{E}\\{P_k(\\xi)P_j(\\xi)\\} = \\int_{-1}^{1} P_k(x)P_j(x)dx \\tag{2}\\]\nis diagonal.\n\n\nLet us write a code that computes the Matrix and then plot the matrix using a heatmap to check if it diagonal.\n\nfunction isdiagonal(π::Vector{Function}, sup::SVector{2})\n    A = Matrix{Float64}(undef, 7, 7)\n    for idx in CartesianIndices(A)\n        (k, j) = idx.I\n        ele = integ(x-&gt; π[k+1](x) * π[j+1](x) * μ(x), sup)\n        A[k, j] = ele &lt; 1e-12 ? 1e-8 : ele\n    end\n    fig = Figure()\n    ax = Axis(fig[1, 1], title = \"heatmap of the diagona matrix\")\n    heatmap!(ax, log10.(A))\n    ax.yreversed=true\n    save(\"heatmapdiagonal.png\", fig)\nend\nisdiagonal(π, sup)\n\n\n\n\nHeatmap showing that the matrix of innerproducts with respect to the weight function of all polynomials generated is diagonal.\n\n\nClearly we can see that the matrix is diagonal and thus the polynomial functions are orthogonal with respect to the weight function."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#part-c",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#part-c",
    "title": "Homework 5",
    "section": "",
    "text": "Plot \\(P_k(x)\\) for \\(k = 0, ..., 6\\)\n\n\nLet us define a code that plots all polynomials \\(P_k(x)\\) for \\(k = 0, ..., 6\\)\n\nfunction plotpolynomials(π::Vector{Function})\n    M = size(π)[1]\n    fig = Figure();display(fig)\n    ax = Axis(fig[1, 1], title = \"Plot of the set of orthogonal polynomials up to degree 6\")\n    xs = LinRange(-1.0, 1.0, 1000)\n    for n in 1:M-1\n        lines!(ax, xs, π[n+1].(xs), label=\"π_$(n-1)\")\n    end\n    Legend(fig[1, 2], ax)\n    save(\"plotpolynomials.png\", fig)\nend\nplotpolynomials(π)\n\n\n\n\nPlot of polynomials within the support [-1, 1]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#part-a-1",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#part-a-1",
    "title": "Homework 5",
    "section": "Part A",
    "text": "Part A\nCompute the PDF of \\(\\eta\\) using the relative frequency approach. To this end, sample 50,000 realizations of \\(\\xi\\) using the inverse CDF approach applied to Equation 1, and use such samples to compute samples of \\(\\eta(\\omega)\\).\n\nSolution\nThe following code samples from Equation 1 50,000 times using the inverse sampling method previously applied in Homework 1 and 2, then applies the transformation Equation 3, and finally plots the histogram with 80 bins, normalized so that the PDF integrates to one over the support.\n\nη(ξ) = (ξ - 1) / (2 + sin(2*ξ))\nfunction question2a()\n    r = LinRange(-1, 1, 1000)\n    fig = Figure();display(fig)\n    ax = Axis(fig[1, 1],\n        title = \"PDF of η(ξ(ω))\")\n    ys = cumsumtrap(μ, r)\n    samples = Vector{Float64}(undef, 50000)\n    for i in eachindex(samples)\n        samples[i] = η(sampleInverseCDF(rand(), hcat(ys, r)))\n    end\n    hist!(ax, samples, bins = 80, normalization = :pdf)\n    save(\"question2a.png\", fig)\n    samples\nend\nsamples = question2a();\n\n\n\n\nPDF of \\(\\eta(\\xi(\\omega))\\) by sampling 50,000 times via inverse CDF approach and applying the transformation Equation 3"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#part-b-1",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#part-b-1",
    "title": "Homework 5",
    "section": "Part B",
    "text": "Part B\nShow numerically that the gPC expansion 1\n\\[\\eta_M(\\omega) = \\sum_{k=0}^{M}a_k P_k(\\xi(\\omega)), \\quad a_k = \\frac{\\mathbb{E}\\{\\eta(\\xi(\\omega))P_k(\\xi(\\omega))\\}}{\\mathbb{E}\\{P_k^2(\\xi(\\omega))\\}}\n\\tag{5}\\]\nconverges to \\(\\eta(\\omega)\\) in distribution as \\(M\\) increases. To this end, plot the PDF of the random variables \\(\\eta_M(\\xi(\\omega))\\) for \\(M = 1, 2, 4, 6\\) using method of relative frequencies and compare such PDF’s with the PDF of \\(\\eta\\) you computed in part a.\n\nSolution\n\nfunction question2b(πn::Vector{Function})\n    a = Vector{Float64}(undef, 7)\n    colors = Symbol[:red, :green, :blue, :yellow, :orange, :purple]\n    # calculate coefficients\n    for k in eachindex(a)\n        a[k] = integ(x -&gt; η(x) * πn[k+1](x) * μ(x), sup) / integ(x -&gt; πn[k+1](x) * πn[k+1](x) * μ(x), sup)\n    end\n\n    fig = Figure();display(fig)\n    ax = Axis(fig[1, 1], title=\"densities of ηM(ξ(ω)) for M = {1, 2, 4, 6}\")\n    r = LinRange(-1, 1, 1000)\n    ys = hcat(cumsumtrap(μ, r), r)\n    for M in SVector{4}(1, 2, 4, 6)\n        ηM_samples = Vector{Float64}(undef, 50000)\n        for l in eachindex(ηM_samples)\n            η_sum = 0.0\n            ξ = sampleInverseCDF(rand(), ys)\n            for k in 1:M+1\n                η_sum+=a[k]*πn[k+1](ξ)\n            end\n            ηM_samples[l] = η_sum\n        end\n        density!(ax, ηM_samples, color = (colors[M], 0.3), label = \"M = $M\", strokecolor = colors[M], strokewidth = 3, strokearound = true)\n    end\n    Legend(fig[1, 2], ax)\n    save(\"question2b.png\", fig)\nend\nquestion2b(π);\n\n\n\n\nThe PDF ploted using Kernel Density Estimation for values M = {1, 2, 4, 6}\n\n\nAs we can see, the PDF of the random variable \\(\\eta_M(\\omega)\\) converges to the PDF of \\(\\eta(\\omega)\\) as M increases."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#part-c-1",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#part-c-1",
    "title": "Homework 5",
    "section": "Part C",
    "text": "Part C\nCompute the mean and variance of \\(\\eta_6\\) and compare it with the mean and variance of \\(\\eta\\). Note that such means and variances can be computed in multiple ways, e.g., by using MC, or by approximating the integral defining the moments of the random variable \\(\\eta\\) using quadrature, e.g., via the trapezoidal rule applied to the integral\n\\[\\mathbb{E}\\{\\eta^k\\} = \\int_{-1}^{1} \\left(\\frac{x-1}{2+\\sin(2x)}\\right)^k p_{\\xi}(x)dx\n\\tag{6}\\]\n\nSolution\nLet us find the mean and variance of \\(\\eta\\) and \\(\\eta_6\\) by generating 1000 samples of size 1000 and taking the average of the mean and variance for each sample.\n\nfunction question2c()\n    πn = stieltjes(μ, 6, sup)\n    r = LinRange(-1, 1, 1000)\n    ys = hcat(cumsumtrap(μ, r), r)\n    η_samples = Vector{Float64}(undef, 50000)\n    for i in eachindex(η_samples)\n        η_samples[i] = η(sampleInverseCDF(rand(), ys))\n    end\n    a = Vector{Float64}(undef, 7)\n    for k in eachindex(a)\n        a[k] = integ(x -&gt; η(x) * πn[k+1](x) * μ(x), sup) / integ(x -&gt; πn[k+1](x) * πn[k+1](x) * μ(x), sup)\n    end\n    η6_samples = Vector{Float64}(undef, 50000)\n    for l in eachindex(η6_samples)\n        η_sum = 0.0\n        ξ = sampleInverseCDF(rand(), ys)\n        for k in eachindex(a)\n            η_sum+=a[k]*πn[k+1](ξ)\n        end\n        η6_samples[l] = η_sum\n    end\n\n    means = Vector{Float64}(undef, 1000)\n    variances = Vector{Float64}(undef, 1000)\n    for i in eachindex(means)\n        sample = rand(η_samples, 1000)\n        means[i] = mean(sample)\n        variances[i] = var(sample)\n    end\n    η_mean = mean(means)\n    η_var = mean(variances)\n\n    means = Vector{Float64}(undef, 1000)\n    variances = Vector{Float64}(undef, 1000)\n    for i in eachindex(means)\n        sample = rand(η6_samples, 1000)\n        means[i] = mean(sample)\n        variances[i] = var(sample)\n    end\n    η6_mean = mean(means)\n    η6_var = mean(variances)\n    fig = Figure();display(fig)\n    ax = Axis(fig[1, 1],\n        xticks = (1:2, [\"η\", \"η₆\"]),\n    title = \"mean and variance for η and η_6\")\n    barplot!(ax, [1, 1, 2, 2], [η_mean, η_var, η6_mean, η6_var],\n        dodge = [1, 2, 1, 2],\n        color = [1, 2, 1, 2])\n    save(\"question2c.png\", fig)\nend\nquestion2c();\n\n\n\n\nBarplot of the bootstrap mean and variance from MC for \\(\\eta\\) and \\(\\eta_6\\)\n\n\nFrom the plot we can see the the sample mean and variance for both are almost identical."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#part-a-2",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#part-a-2",
    "title": "Homework 5",
    "section": "Part A",
    "text": "Part A\nCompute the mean and the variance of Equation 8 of \\(t \\in [0, 3]\\).\n\nSolution\nLet us find the solution of the random initial value problem by using the stocastic Galerkin method and finding the gPC modes by plugging in Equation 8 into Equation 7.\n\\[\\begin{align}\n    \\sum_{k=0}^{6}\\frac{\\partial \\hat{x}_k}{\\partial t}P_k &= -\\xi \\sum_{k=0}^{6}\\hat{x}_k P_k + cos(4t) \\\\\n\\frac{\\partial \\hat{x}_j}{\\partial t}\\mathbb{E}\\{P_j^2\\} &= -\\sum_{k=0}^{6}\\hat{x}_k \\mathbb{E}\\{\\xi P_k P_j\\} + \\cos(4t) \\mathbb{E}\\{P_j\\} \\\\\n\\frac{\\partial \\hat{x}_j}{\\partial t} &= -\\frac{1}{\\mathbb{E}\\{P_j^2\\}}\\sum_{k=0}^{6}\\hat{x}_k \\mathbb{E}\\{\\xi P_k P_j\\} + \\frac{\\cos(4t) \\mathbb{E}\\{P_j\\}}{\\mathbb{E}\\{P_j^2\\}} \\\\\n\\end{align}\\]\nRecall from Question 1, Part A where we calculated the \\(P_1(\\xi)\\)\n\\[\\begin{align}\nP_1(\\xi) &= \\xi + \\frac{2}{e^2 + 1} \\\\\n\\xi &= P_1(\\xi) - \\frac{2}{e^2 + 1}\n\\end{align}\\]\nPlugging \\(\\xi\\) back into the ODE we have:\n\\[\\begin{align}\n\\frac{\\partial \\hat{x}_j}{\\partial t} &= -\\frac{1}{\\mathbb{E}\\{P_j^2\\}}\\sum_{k=0}^{6}\\hat{x}_k \\mathbb{E}\\{ \\left(P_1(\\xi) - \\frac{2}{e^2 + 1}\\right) P_k P_j\\} + \\frac{\\cos(4t) \\mathbb{E}\\{P_j\\}}{\\mathbb{E}\\{P_j^2\\}} \\\\\n\\frac{\\partial \\hat{x}_j}{\\partial t} &= -\\frac{1}{ \\mathbb{E}\\{P_j^2\\}} \\sum_{k=0}^{6}\\hat{x}_k \\mathbb{E}\\{P_1 P_k P_j\\} + \\frac{2\\hat{x}}{e^2 + 1} + \\frac{\\cos(4t) \\mathbb{E}\\{P_j\\}}{ \\mathbb{E}\\{P_j^2\\}}\n\\end{align}\\]\nwhere the initial conditions are:\n\\[\\begin{align}\nx(0;\\omega) &= 1 \\\\\nx(j;\\omega) &= 0 \\quad \\text{for } j = 1, ..., 6\n\\end{align}\\]\nIn the following code we precompute the expectations required for defining the first order system of equations, solve the IVP using Tsitouras 5/4 Runge-Kutta method, calculate the variance by the following:\n\\[\\begin{align}\nvar(x(t;\\omega)) = \\sum_{k=1}^{6}\\hat{x}_k^2(t) \\mathbb{E}\\{P_k^2\\}\n\\end{align}\\]\nand plot the mean and variance on two seperate graphs in the temporal domain [0, 3]\n\nusing DifferentialEquations\nfunction question3a()\n    # get polynomials\n    πn = stieltjes(μ, 6, sup)\n\n    # simulation initial and final time\n    t0, tf = SVector{2}(0.0, 3.0)\n\n    # initial conditions\n    u0 = SVector{7}(1, 0, 0, 0, 0, 0, 0)\n\n    # highest polynomial order\n    M = size(πn)[1] - 1\n\n    # precompute E{Pj}\n    Epj1 = Vector{Float64}(undef, M)\n    for k in eachindex(Epj1)\n        Epj1[k] = integ(x -&gt; πn[k+1](x) * μ(x), sup)\n    end\n\n    # precompute E{Pj^2}\n    Epj2 = Vector{Float64}(undef, M)\n    for j in eachindex(Epj2)\n            Epj2[j] = integ(x -&gt; πn[j+1](x) * πn[j+1](x) * μ(x), sup)\n    end\n\n    # precompute E{P₁PkPj}\n    Ep1pjpk = Matrix{Float64}(undef, M, M)\n    for idx in CartesianIndices(Ep1pjpk)\n        (j, k) = idx.I\n        Ep1pjpk[j, k] = integ(x -&gt; πn[2](x) * πn[j+1](x) * πn[k](x) * μ(x), sup)\n    end\n\n    # precompute the constant multiplying xhatj\n    β = (2.0 / (exp(1.0)^2 + 1.0))\n\n    # define the system of odes\n    function ode(u, p, t)\n        return SVector{M}(\n            ((-sum(u[k] * Ep1pjpk[j, k] for k in 1:M))/Epj2[j]) + β*u[j] + (cos(4*t)*Epj1[j]/Epj2[j]) for j in 1:M\n        )\n    end\n\n    # solve the ode problem using Tsitouras 5/4 Runge-Kutta method\n    prob = ODEProblem(ode, u0, (t0, tf))\n    sol = solve(prob, Tsit5(), saveat=0.01, abstol=1e-8, reltol=1e-8)\n    # find the variance\n    var = sum([(sol[k, :].^2) .* Epj2[k] for k in 2:M], dims=1)[1]\n\n    # plot the mean and variance\n    fig = Figure(size = (800, 400));display(fig)\n    ax1 = Axis(fig[1, 1], title = \"mean of x(t;ω)\",\n        xlabel = \"time\")\n    ax2 = Axis(fig[1, 2], title = \"variance of x(t;ω)\",\n        xlabel = \"time\")\n    lines!(ax1, sol.t, sol[1, :])\n    lines!(ax2, sol.t, var)\n    save(\"question3a.png\", fig)\n    return sol\nend\nquestion3a();\n\n\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n\n\n\n\n\n\n\nThe mean and variance of Equation 8 in \\(t \\in [0, 3]\\)"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#part-b-2",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#part-b-2",
    "title": "Homework 5",
    "section": "Part B",
    "text": "Part B\nCompute the PDF of Equation 8 at times \\(\\{0.5, 1, 2, 3\\}\\) (use relative frequencies).\nNote: you can debug your gPC results by either computing the analytic solution of Equation 7 and then computing moments/PDFs of such solutions as a function of \\(t\\), or by randomly sampling many solution paths of Equation 7 and then computing ensemble averages.\n\nSolution\nWe have everything we need to compute the PDF of Equation 8 for various times.\nLet us generate a random vector by sampling \\(p_{\\xi}(x)\\) 50,000 times, then generate samples by evaluating the following:\n\\[\\begin{align}\nx(t^*;\\omega) = \\sum_{i=0}^{6}\\hat{x}_i(t^*) P_i(\\xi(\\omega))\n\\end{align}\\]\nwhere\n\\[\\begin{align}\n&\\hat{x}_i(t^*) &\\quad &\\text{are the gPC coefficients at time } t^* \\\\\n&P_i(\\xi) &\\quad &\\text{are the gPC basis functions} \\\\\n&\\xi &\\quad &\\text{random variables from the distribution } P_{\\xi}(x)\n\\end{align}\\]\n\nfunction question3b()\n    # get orthogonal polynomials\n    πn = stieltjes(μ, 6, sup)\n\n    # get gPC modes\n    sol = question3a()\n\n    # generate samples of ξ\n    r = LinRange(-1, 1, 1000)\n    ys = hcat(cumsumtrap(μ, r), r)\n    ξ_samples = Vector{Float64}(undef, 50000)\n    for i in eachindex(ξ_samples)\n        ξ_samples[i] = sampleInverseCDF(rand(), ys)\n    end\n\n    # define t*\n    times = SVector{4}(0.5, 1.0, 2.0, 3.0)\n\n    # number of solutions (gPC modes)\n    M = size(sol)[1]\n\n    # define figure\n    fig = Figure(size = (800, 800))\n    ax = SVector{4}(Axis(fig[1, 1]), Axis(fig[1, 2]), Axis(fig[2, 1]), Axis(fig[2, 2]))\n\n    # plot histogram of samples of x(tstar;ω)\n    for (i, tstar) in enumerate(times)\n        hist!(ax[i], sum(sol(tstar)[j] .* πn[j].(ξ_samples) for j in 1:M), bins = 80, normalization = :pdf)\n        ax[i].title = \"t = $tstar\"\n        ax[i].ylabel = \"normalized frequency\"\n    end\n    Label(fig[0, :], \"PDF of various times, for x(t;ω)\", fontsize=20)\n    save(\"question3b.png\", fig)\nend\nquestion3b();\n\n\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n\n\n\n\n\n\n\nPDF for various times found by the method of relative frequencies"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#validation",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#validation",
    "title": "Homework 5",
    "section": "Validation",
    "text": "Validation\nLet us randomly sample many solution paths of Equation 7 and then compute the ensemble average to validate our gPC results.\nFor this we will generate 50,000 samples from the PDF of Equation 1, then solve the ODE numerically 50,000 times and plot the histogram of the normalized frequency just as we did in Question 3 partb.\n\nfunction validation(N::Int64)\n    # generate samples of ξ\n    r = LinRange(-1, 1, 1000)\n    ys = hcat(cumsumtrap(μ, r), r)\n    ξ_samples = Vector{Float64}(undef, N)\n    for i in eachindex(ξ_samples)\n        ξ_samples[i] = sampleInverseCDF(rand(), ys)\n    end\n\n    # define t*\n    times = SVector{4}(0.5, 1.0, 2.0, 3.0)\n\n    # define the ODEProblem\n    function ode(u, p, t)\n        return SVector{1}(-p * u[1] + cos(4*t))\n    end\n\n    # initial condition\n    u0 = SVector{1}(1)\n\n    # solve once to get size of time vector\n    prob = ODEProblem(ode, u0, (0.0, 3.0), -1.0)\n    sol = solve(prob, Tsit5(), saveat=0.01, abstol=1e-8, reltol=1e-8)\n\n    # init matrix to store solutions\n    solTstar = [Float64[] for _ in 1:length(times)]\n    solutions = Matrix{Float64}(undef, N, size(sol.t)[1])\n\n    # solve the ode for each sample and extract sample paths\n    for (i, ξ) in enumerate(ξ_samples)\n        prob = ODEProblem(ode, u0, (0.0, 3.0), ξ)\n        sol = solve(prob, Tsit5(), saveat=0.01, abstol=1e-8, reltol=1e-8)\n        solutions[i, :] = Float64[u[1] for u in sol.u]\n        for (j, tstar) in enumerate(times)\n            push!(solTstar[j], sol(tstar)[1])\n        end\n    end\n\n    # calculate sample path mean and variance\n    meanPath = mean(solutions, dims=1)\n    variancePath = var(solutions, mean=meanPath, dims=1)\n\n    # plot results\n    fig1 = Figure(size = (800, 800))\n    ax1 = [Axis(fig1[i, j]) for i in 1:2, j in 1:2]\n    fig2 = Figure(size = (800, 400))\n    ax2 = [Axis(fig2[1, i]) for i in 1:2]\n    for (i, tstar) in enumerate(times)\n        hist!(ax1[i], solTstar[i], bins = 80, normalization = :pdf)\n        ax1[i].title = \"t = $tstar\"\n        ax1[i].ylabel = \"Normalized Frequency\"\n    end\n    Label(fig1[0, :], \"PDF of Various Times Using MC Method\", fontsize=20)\n    lines!(ax2[1], sol.t, vec(meanPath), label=\"Mean Path\")\n    ax2[1].title = \"Mean Path\"\n    ax2[1].xlabel = \"Time\"\n    lines!(ax2[2], sol.t, vec(variancePath), label=\"Variance\")\n    ax2[2].title = \"Variance over Time\"\n    ax2[2].xlabel = \"Time\"\n    save(\"validationPDF.png\", fig1)\n    save(\"validationmeanvar.png\", fig2)\nend\nvalidation(50000);\n\n\n\n\nThe mean and variance of \\(x(t;\\omega)\\) using Monte Carlo method\n\n\n\n\n\nThe PDF for specific times using Monte Carlo method"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw5/hw5.html#footnotes",
    "href": "courses/uncertainty_quant/docs/hw5/hw5.html#footnotes",
    "title": "Homework 5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that \\(\\mathbb{E}\\{\\eta(\\xi(\\omega))P_k(\\xi(\\omega))\\}\\) can be computed with MC, or with quadrature as \\[\\mathbb{E}\\{\\eta(\\xi(\\omega))P_k(\\xi(\\omega))\\}= \\int_{-1}^{1}\\frac{x - 1}{2 + 1 \\sin(2x)}P_k(x)P_\\xi(x)dx \\tag{4}\\]↩︎"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html",
    "href": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html",
    "title": "Problem",
    "section": "",
    "text": "In DNA strings, sysmbols ‘A’ and ‘T’ are complements of each other, as are ‘C’ and ‘G’.\nThe reverse complement of a DNA string \\(s\\) is the string \\(s^C\\) formed by reversing the symbols of \\(s\\), then taking the complement of each symbol (e.g., the reverse complement of ‘GTCA’ is ‘TGAC’).\n\n\nA DNA string \\(s\\) of length at most 1000 base pairs.\n\n\n\nThe reverse complement \\(s^C\\) of \\(s\\).\n\n\n\nAAAACCCGGT\n\n\n\nACCGGGTTTT\n\n\n\nfunction reverse_complement(s::AbstractString)\n    rev_map = Dict{Char, Char}(\n        'A' =&gt; 'T', 'T' =&gt; 'A',\n        'G' =&gt; 'C', 'C' =&gt; 'G'\n    )\n    io = IOBuffer()\n    for char in reverse(s)\n        write(io, rev_map[char])\n    end\n    return String(take!(io))\nend\n\nfunction main()\n    first_line = readline(ARGS[1])\n    output = reverse_complement(first_line)\n    write(\"output.txt\", output)\n    println(output)\nend\n\nmain()"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html#given",
    "href": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html#given",
    "title": "Problem",
    "section": "",
    "text": "A DNA string \\(s\\) of length at most 1000 base pairs."
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html#return",
    "href": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html#return",
    "title": "Problem",
    "section": "",
    "text": "The reverse complement \\(s^C\\) of \\(s\\)."
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html#sample-dataset",
    "href": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html#sample-dataset",
    "title": "Problem",
    "section": "",
    "text": "AAAACCCGGT"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html#sample-output",
    "href": "rosalind/bioinformatics_stronghold/complementing_a_strand_of_dna/complement.html#sample-output",
    "title": "Problem",
    "section": "",
    "text": "ACCGGGTTTT\n\n\n\nfunction reverse_complement(s::AbstractString)\n    rev_map = Dict{Char, Char}(\n        'A' =&gt; 'T', 'T' =&gt; 'A',\n        'G' =&gt; 'C', 'C' =&gt; 'G'\n    )\n    io = IOBuffer()\n    for char in reverse(s)\n        write(io, rev_map[char])\n    end\n    return String(take!(io))\nend\n\nfunction main()\n    first_line = readline(ARGS[1])\n    output = reverse_complement(first_line)\n    write(\"output.txt\", output)\n    println(output)\nend\n\nmain()"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html",
    "href": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html",
    "title": "Problem",
    "section": "",
    "text": "Write a program\nGiven: A DNA string \\(t\\) having length at most 1000 nucleotides\nReturn: The transcribed RNA string of \\(t\\).\n\n\nGATGGAACTTGACTACGTAAATT\n\n\n\nGAUGGAACUUGACUACGUAAAUU\n\n\n\ntranscription.jl\n\ntranscribe(s::AbstractString) = replace(s, 'T'=&gt;'U')\n\nfunction main()\n    if length(ARGS) &lt; 2\n        println(\"Usage: julia $(Base.PROGRAM_FILE) &lt;fineIN&gt; &lt;fileOUT&gt;\")\n        exit(1)\n    end\n\n    s = read(ARGS[1], String)\n\n    open(ARGS[2], \"w\") do file\n        println(file, transcribe(s))\n    end\n    exit(0)\nend\n\nmain()\n\n\n\n\nWe use the in-built function replace which takes in a collection and for each pair old=&gt;new returns a copy of the collection where all occurances of old are replaced by new.\nGithub Files\nWe download the rosalind dataset and ran the code using,\njulia transcription.jl rosalind_rna.txt output.txt"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html#sample-data",
    "href": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html#sample-data",
    "title": "Problem",
    "section": "",
    "text": "GATGGAACTTGACTACGTAAATT"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html#sample-output",
    "href": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html#sample-output",
    "title": "Problem",
    "section": "",
    "text": "GAUGGAACUUGACUACGUAAAUU"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html#code",
    "href": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html#code",
    "title": "Problem",
    "section": "",
    "text": "transcription.jl\n\ntranscribe(s::AbstractString) = replace(s, 'T'=&gt;'U')\n\nfunction main()\n    if length(ARGS) &lt; 2\n        println(\"Usage: julia $(Base.PROGRAM_FILE) &lt;fineIN&gt; &lt;fileOUT&gt;\")\n        exit(1)\n    end\n\n    s = read(ARGS[1], String)\n\n    open(ARGS[2], \"w\") do file\n        println(file, transcribe(s))\n    end\n    exit(0)\nend\n\nmain()"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html#notes",
    "href": "rosalind/bioinformatics_stronghold/transcribing_dna_into_rna/transcription.html#notes",
    "title": "Problem",
    "section": "",
    "text": "We use the in-built function replace which takes in a collection and for each pair old=&gt;new returns a copy of the collection where all occurances of old are replaced by new.\nGithub Files\nWe download the rosalind dataset and ran the code using,\njulia transcription.jl rosalind_rna.txt output.txt"
  },
  {
    "objectID": "rosalind/index.html",
    "href": "rosalind/index.html",
    "title": "Rosalind",
    "section": "",
    "text": "Rosalind is a platform for learning bioinformatics through problem solving. The following are my solutions to the problems posed using the Julia programming language.\n\n\nCounting DNA Nucleotides\nTranscribing DNA into RNA\nComplementing a Strand of DNA"
  },
  {
    "objectID": "rosalind/index.html#bioinformatics-stronghold",
    "href": "rosalind/index.html#bioinformatics-stronghold",
    "title": "Rosalind",
    "section": "",
    "text": "Counting DNA Nucleotides\nTranscribing DNA into RNA\nComplementing a Strand of DNA"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/counting_dna_nucleotides/countdna.html",
    "href": "rosalind/bioinformatics_stronghold/counting_dna_nucleotides/countdna.html",
    "title": "Problem",
    "section": "",
    "text": "Given: A DNA string s of length \\(1000\\) nucleotides\nReturn: Four integers (seperated by spaces) counting the respoective number of times that the symbols ‘A’ ‘C’ ‘G’ ‘T’ occur in s.\nSample Dataset:\nAGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGC\nSample Output:\n\\[\\begin{matrix}\n20 & 12 & 17 & 21\n\\end{matrix}\\]\n\n\nWe use the inbuilt julia function count(p, iter) which counts the number of elements in iter for which predicate p is true.\ncount\n\ncountNucs(s::String) = [\n    count(==('A'), s), count(==('C'), s), count(==('G'), s), count(==('T'), s)\n]\n\nfunction main()\n    if length(ARGS) &lt; 2\n        println(\"Usage: julia $(Base.PROGRAM_FILE) &lt;fileIN&gt; &lt;fileOUT&gt;\")\n        exit(1)\n    end\n    s = read(ARGS[1], String)\n    data = countNucs(s)\n    open(ARGS[2], \"w\") do file\n        println(file, join(data, \" \"))\n    end\n    exit(0)\nend\n\nmain()\n\n\n\n\nIn Rosalind, you have to ‘download the dataset’ and they give you five minuets to submit your answer. This essentially means that your solution nees to be reasonably computationally efficient to solve within that time frame. If you write bad code, sometimes this is a non-trivial requirement because their data files for testing contain a large number of data.\nGithub Files\nThe cold was ran by downloading the dataset, then running the following command:\njulia countdna.jl rosalind_dna.txt output.txt"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/counting_dna_nucleotides/countdna.html#solution",
    "href": "rosalind/bioinformatics_stronghold/counting_dna_nucleotides/countdna.html#solution",
    "title": "Problem",
    "section": "",
    "text": "We use the inbuilt julia function count(p, iter) which counts the number of elements in iter for which predicate p is true.\ncount\n\ncountNucs(s::String) = [\n    count(==('A'), s), count(==('C'), s), count(==('G'), s), count(==('T'), s)\n]\n\nfunction main()\n    if length(ARGS) &lt; 2\n        println(\"Usage: julia $(Base.PROGRAM_FILE) &lt;fileIN&gt; &lt;fileOUT&gt;\")\n        exit(1)\n    end\n    s = read(ARGS[1], String)\n    data = countNucs(s)\n    open(ARGS[2], \"w\") do file\n        println(file, join(data, \" \"))\n    end\n    exit(0)\nend\n\nmain()"
  },
  {
    "objectID": "rosalind/bioinformatics_stronghold/counting_dna_nucleotides/countdna.html#notes",
    "href": "rosalind/bioinformatics_stronghold/counting_dna_nucleotides/countdna.html#notes",
    "title": "Problem",
    "section": "",
    "text": "In Rosalind, you have to ‘download the dataset’ and they give you five minuets to submit your answer. This essentially means that your solution nees to be reasonably computationally efficient to solve within that time frame. If you write bad code, sometimes this is a non-trivial requirement because their data files for testing contain a large number of data.\nGithub Files\nThe cold was ran by downloading the dataset, then running the following command:\njulia countdna.jl rosalind_dna.txt output.txt"
  },
  {
    "objectID": "courses/uncertainty_quant/index.html",
    "href": "courses/uncertainty_quant/index.html",
    "title": "Fundamentals of Uncertainty Quantification in Computational Science and Engineering",
    "section": "",
    "text": "Computing the statistical properties of nonlinear random systems is of fundamental importance in many areas of science and engineering. The primary objective of the course is to introduce students to state-of-the-art methods for uncertainty propagation and quantification in model-based computations, focusing on the computational and algorithmic features of these methods most useful in dealing with systems specified in terms of ordinary and partial differential equations.\nThe course will focus mainly on the so-called forward UQ problem, in which uncertainties in input parameters such as initial conditions, boundary conditions, geometry or forcing terms are propagated through the equations of motion of the system into the solution. The course will also discuss cutting edge topics on data-driven modeling, deep learning with stochastic neural networks, and uncertainty propagation in high-dimensional systems via tensor methods.\nInstructor: Prof. Daniele Venturi\n\n\n\nProbability Spaces and Random Variables\nRandom Vectors\nRandom Processes and Random Fields\nPDF equations for dynamical systems and PDEs\nPolynomial Chaos\nSampling Methods\n\n\n\nThe following are my solutions to the homework assignments presented to me in this class.\nHomework 1\nHomework 2\nHomework 3\nHomework 4\nHomework 5\nFinal Project"
  },
  {
    "objectID": "courses/uncertainty_quant/index.html#course-description",
    "href": "courses/uncertainty_quant/index.html#course-description",
    "title": "Fundamentals of Uncertainty Quantification in Computational Science and Engineering",
    "section": "",
    "text": "Computing the statistical properties of nonlinear random systems is of fundamental importance in many areas of science and engineering. The primary objective of the course is to introduce students to state-of-the-art methods for uncertainty propagation and quantification in model-based computations, focusing on the computational and algorithmic features of these methods most useful in dealing with systems specified in terms of ordinary and partial differential equations.\nThe course will focus mainly on the so-called forward UQ problem, in which uncertainties in input parameters such as initial conditions, boundary conditions, geometry or forcing terms are propagated through the equations of motion of the system into the solution. The course will also discuss cutting edge topics on data-driven modeling, deep learning with stochastic neural networks, and uncertainty propagation in high-dimensional systems via tensor methods.\nInstructor: Prof. Daniele Venturi"
  },
  {
    "objectID": "courses/uncertainty_quant/index.html#course-notes",
    "href": "courses/uncertainty_quant/index.html#course-notes",
    "title": "Fundamentals of Uncertainty Quantification in Computational Science and Engineering",
    "section": "",
    "text": "Probability Spaces and Random Variables\nRandom Vectors\nRandom Processes and Random Fields\nPDF equations for dynamical systems and PDEs\nPolynomial Chaos\nSampling Methods"
  },
  {
    "objectID": "courses/uncertainty_quant/index.html#homework-solutions",
    "href": "courses/uncertainty_quant/index.html#homework-solutions",
    "title": "Fundamentals of Uncertainty Quantification in Computational Science and Engineering",
    "section": "",
    "text": "The following are my solutions to the homework assignments presented to me in this class.\nHomework 1\nHomework 2\nHomework 3\nHomework 4\nHomework 5\nFinal Project"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Consider a Gaussian random process \\(X(t;\\omega)\\) defined on the time interval [0, 5]. The process has mean \\[\\mu(t) = t e^{\\sin{(3t)}}\\] and covariance function \\[cov(t, s) = e^{\\frac{-|t-s|}{\\tau}}\\] where \\(\\tau &gt; 0\\) represents the temporal “correlation length” of the Gaussian process.\n\n\nCompute the standard deviation of \\(X(t;\\omega)\\) at time \\(t\\).\n\n\nNotice that when \\(t = s\\), the covariance is 1 because we have \\(e^0\\). The variance of the random variable \\(X(t;\\omega)\\) at any particular time \\(t\\) is equal to the Covariance of \\(t\\) with itself. Since the standard deviation is just the square root of the variance we can easily show that:\n\\[\\begin{align}\n    \\sigma(t) = \\sqrt{Var(X(t;\\omega))} = \\sqrt{Cov(t, t)} = \\sqrt{e^{\\frac{-|t - t|}{\\tau}}} = \\sqrt{e^0} = 1\n\\end{align}\\]\n\n\n\n\nCompute the covariance matrix of the random variables \\(X(1;\\omega)\\) and \\(X(2;\\omega)\\) as a function of \\(\\tau\\). What happens when \\(\\tau \\rightarrow 0\\)?\n\n\nTherefore, the covariance is always 1 along the diagonal elements of the covariance matrix \\(\\Sigma\\). Additionally, the off-diagonal elements are equal because we are taking the difference between the absolute values of \\(t\\) and \\(s\\). Thus, the matrix is symmetric.\n\\[\\begin{align}\n    \\Sigma_X(\\tau) = \\left[\n    \\begin{matrix}\n        1.0 & e^{\\frac{-|1-2|}{\\tau}} \\\\\n        e^{\\frac{-|2-1|}{\\tau}} & 1.0\n    \\end{matrix}\\right]\n\\end{align}\\]\nFor \\(\\Sigma(\\tau \\rightarrow 0) \\rightarrow \\mathbf{I}\\) where \\(\\mathbf{I}\\) is the identity matrix.\n\\[\\begin{align}\n    \\lim_{\\tau \\rightarrow \\infty} \\Sigma_X(\\tau) = \\left[\\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix}\\right] = \\mathbf{I}\n\\end{align}\\]\n\n\n\n\nPlot a few samples of \\(X(t;\\omega)\\) for \\(\\tau = 0.02\\) and \\(\\tau = 1\\) on a temporal with \\(5000\\) points in [0, 5] (two different figures). Show that such sample paths are approximately within \\(\\mu(t) \\pm 2\\sigma(t)\\), where \\(\\sigma(t)\\) is the standard deviation of the process.\n\n\nFirst, we define a 1D grid for time from 0 to 5, equally spaced with 5000 points. We then define the functions for the mean and covariance and construct a vector of means corresponding to each point in time. Next, we construct a covariance matrix, perform the lower triangular Cholesky factorization of that matrix, and generate a random vector. \\[\\bar{X_i} = \\bar{\\mu}_i + \\bar{A}\\bar{\\xi}\\] where \\(\\xi \\sim \\mathcal{N}(0, 1)\\)\n\nusing GLMakie\nusing Distributions\nusing LinearAlgebra\nusing KernelDensity\nusing SpecialFunctions\n\nfunction makefig1(τ::Float64)\n    t = LinRange(0.0, 5.0, 1000)\n    μ(t) = t*exp(sin(3*t))\n    cov(t, s) = exp((-abs(t-s)) / (τ))\n    μs = μ.(t)\n    Σ = Matrix{Float64}(undef, length(t), length(t))\n    for idx in CartesianIndices(Σ)\n        Σ[idx] = cov(t[idx.I[1]], t[idx.I[2]])\n    end\n    A = cholesky(Σ).L\n    fig = Figure()\n    ax = Axis(fig[1, 1])\n    for i in 1:5\n        Xi = μs .+ A*randn(length(t))\n        lines!(t, Xi)\n    end\n    lines!(ax, t, μs, color = :red, label = \"μ\")\n    lines!(ax, t, (μs .+ 2),\n           color = :black,\n           linestyle = :dash)\n    lines!(ax, t, (μs .+ -2),\n           color = :black,\n           linestyle = :dash,\n           label = \"μ ± 2\")\n    Legend(fig[1, 2], ax)\n    save(\"question1c_$τ.png\", fig)\nend\nmakefig1(0.02);\nmakefig1(1.0);\n\n\n\n\n\n\n\n\n\n\nFigure 1: \\(\\tau = 0.02\\)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: \\(\\tau = 1.0\\)"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-a",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-a",
    "title": "Homework 3",
    "section": "",
    "text": "Compute the standard deviation of \\(X(t;\\omega)\\) at time \\(t\\).\n\n\nNotice that when \\(t = s\\), the covariance is 1 because we have \\(e^0\\). The variance of the random variable \\(X(t;\\omega)\\) at any particular time \\(t\\) is equal to the Covariance of \\(t\\) with itself. Since the standard deviation is just the square root of the variance we can easily show that:\n\\[\\begin{align}\n    \\sigma(t) = \\sqrt{Var(X(t;\\omega))} = \\sqrt{Cov(t, t)} = \\sqrt{e^{\\frac{-|t - t|}{\\tau}}} = \\sqrt{e^0} = 1\n\\end{align}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-b",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-b",
    "title": "Homework 3",
    "section": "",
    "text": "Compute the covariance matrix of the random variables \\(X(1;\\omega)\\) and \\(X(2;\\omega)\\) as a function of \\(\\tau\\). What happens when \\(\\tau \\rightarrow 0\\)?\n\n\nTherefore, the covariance is always 1 along the diagonal elements of the covariance matrix \\(\\Sigma\\). Additionally, the off-diagonal elements are equal because we are taking the difference between the absolute values of \\(t\\) and \\(s\\). Thus, the matrix is symmetric.\n\\[\\begin{align}\n    \\Sigma_X(\\tau) = \\left[\n    \\begin{matrix}\n        1.0 & e^{\\frac{-|1-2|}{\\tau}} \\\\\n        e^{\\frac{-|2-1|}{\\tau}} & 1.0\n    \\end{matrix}\\right]\n\\end{align}\\]\nFor \\(\\Sigma(\\tau \\rightarrow 0) \\rightarrow \\mathbf{I}\\) where \\(\\mathbf{I}\\) is the identity matrix.\n\\[\\begin{align}\n    \\lim_{\\tau \\rightarrow \\infty} \\Sigma_X(\\tau) = \\left[\\begin{matrix} 1 & 0 \\\\ 0 & 1 \\end{matrix}\\right] = \\mathbf{I}\n\\end{align}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-c",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-c",
    "title": "Homework 3",
    "section": "",
    "text": "Plot a few samples of \\(X(t;\\omega)\\) for \\(\\tau = 0.02\\) and \\(\\tau = 1\\) on a temporal with \\(5000\\) points in [0, 5] (two different figures). Show that such sample paths are approximately within \\(\\mu(t) \\pm 2\\sigma(t)\\), where \\(\\sigma(t)\\) is the standard deviation of the process.\n\n\nFirst, we define a 1D grid for time from 0 to 5, equally spaced with 5000 points. We then define the functions for the mean and covariance and construct a vector of means corresponding to each point in time. Next, we construct a covariance matrix, perform the lower triangular Cholesky factorization of that matrix, and generate a random vector. \\[\\bar{X_i} = \\bar{\\mu}_i + \\bar{A}\\bar{\\xi}\\] where \\(\\xi \\sim \\mathcal{N}(0, 1)\\)\n\nusing GLMakie\nusing Distributions\nusing LinearAlgebra\nusing KernelDensity\nusing SpecialFunctions\n\nfunction makefig1(τ::Float64)\n    t = LinRange(0.0, 5.0, 1000)\n    μ(t) = t*exp(sin(3*t))\n    cov(t, s) = exp((-abs(t-s)) / (τ))\n    μs = μ.(t)\n    Σ = Matrix{Float64}(undef, length(t), length(t))\n    for idx in CartesianIndices(Σ)\n        Σ[idx] = cov(t[idx.I[1]], t[idx.I[2]])\n    end\n    A = cholesky(Σ).L\n    fig = Figure()\n    ax = Axis(fig[1, 1])\n    for i in 1:5\n        Xi = μs .+ A*randn(length(t))\n        lines!(t, Xi)\n    end\n    lines!(ax, t, μs, color = :red, label = \"μ\")\n    lines!(ax, t, (μs .+ 2),\n           color = :black,\n           linestyle = :dash)\n    lines!(ax, t, (μs .+ -2),\n           color = :black,\n           linestyle = :dash,\n           label = \"μ ± 2\")\n    Legend(fig[1, 2], ax)\n    save(\"question1c_$τ.png\", fig)\nend\nmakefig1(0.02);\nmakefig1(1.0);\n\n\n\n\n\n\n\n\n\n\nFigure 1: \\(\\tau = 0.02\\)\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: \\(\\tau = 1.0\\)"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-a-1",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-a-1",
    "title": "Homework 3",
    "section": "Part A",
    "text": "Part A\nWrite the Fokker-Planck (FKP) equation corresponding to the SDE ().\n\nSolution\nFrom course notes 3, we know that the Fokker-Planck equation to the general SDE\n\\[\\begin{align}\n    dX_t = m(X_t, t)dt + s(X_t, t)dW_t \\quad X(0) = X_0\n\\end{align}\\]\nis\n\\[\\begin{align}\n    \\frac{\\partial p(x, t)}{\\partial t} + \\frac{\\partial}{\\partial x} \\left[m(x, t)p(x, t)\\right] = \\frac{1}{2} \\frac{\\partial^2}{\\partial x^2} \\left[s(x, t)^2p(x, t)\\right]\n\\end{align}\\]\nfor equation (Equation 1) we can see the \\(m(x, t) = -X_t^3\\) and \\(s(x, t) = \\frac{1}{2}\\), thus the Fokker-Plank equation for (Equation 1) is\n\\[\\begin{align}\n    \\frac{\\partial p}{\\partial t} + \\frac{\\partial}{\\partial x} \\left[-x^3 p(x, t)\\right] = \\frac{1}{8} \\frac{\\partial^2p}{\\partial x^2}\n\\end{align}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-b-1",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-b-1",
    "title": "Homework 3",
    "section": "Part B",
    "text": "Part B\nUsing the FKP equation show that there exists a statistically stationary solution and compute the PDF \\(p^*(x)\\) of such a stationary solution analytically. Is the equilibrium distribution \\(p^*(x)\\) Gaussian?\nHint: To compute a stationary solution to the Fokker-Planck equation, set the time derivative \\(\\frac{\\partial p}{\\partial t} = 0\\).\n\nSolution\nWe are left to solve the following DE:\n\\[\\begin{align}\n    \\frac{\\partial}{\\partial x} \\left( x^3 p + \\frac{1}{8} \\frac{\\partial p}{\\partial x}\\right) &= 0 \\\\\n    x^3 p + \\frac{1}{8} \\frac{\\partial p}{\\partial x} &= 0 \\\\\n    8 \\int x^3 dx &= - \\int \\frac{1}{p} dp\\\\\n    2x^4 &= -\\ln{p} + k\\\\\n    p(x) &= ke^{-2x^4}\n\\end{align}\\]\nWe need to find a scaling constant \\(k\\) for the pdf \\(p(x, t)\\) such that when you integrate from \\([-\\infty, \\infty]\\) you get \\(1\\).\n\\[\\begin{align}\n    k\\int_{-\\infty}^{\\infty} e^{-2x^4}dx &= 1\n\\end{align}\\]\nNotice the function is even and thus we can write the integral as such:\n\\[\\begin{align}\n    2k\\int_{0}^{\\infty} e^{-2x^4}dx &= 1 \\quad \\begin{cases} u &= x^4 \\rightarrow u^{-\\frac{3}{4}} = x^{-3}\\\\ \\frac{du}{dx} &= 4x^3 \\\\ \\frac{1}{4}u^{-\\frac{3}{4}}du &= dx\\end{cases} \\\\\n    \\frac{K}{2} \\int_0^{\\infty} e^{-2u}u^{\\frac{1}{4} -1} du &= 1 \\quad \\begin{cases}\\xi &= 2u \\\\ \\frac{d\\xi}{2} &= du\\end{cases} \\\\\n    \\frac{K}{4} \\int_0^{\\infty} e^{-\\xi} \\left(\\frac{\\xi}{2}\\right)^{-\\frac{3}{4}} d\\xi &= 1\\\\\n    \\frac{2^{\\frac{3}{4}}K}{4} \\int_0^{\\infty} e^{-\\xi} \\xi^{\\frac{1}{4} - 1} d\\xi\n\\end{align}\\]\nThe Gamma function is \\[\\Gamma(z) = \\int_0^{\\infty} e^{-\\xi}\\xi^{z-1}d\\xi\\]\nSolving for \\(K\\) we have \\[\\begin{align}\n    K\\frac{\\Gamma\\left(\\frac{1}{4}\\right)}{2\\sqrt[4]{2}} &= 1\\\\\n    K &= \\frac{2\\sqrt[4]{2}}{\\Gamma\\left(\\frac{1}{4}\\right)}\n\\end{align}\\]\nThe PDF \\(p^*(x)\\) is thus,\n\\[p^*(x) = \\frac{2\\sqrt[4]{2}}{\\Gamma\\left(\\frac{1}{4}\\right)}e^{-2x^4}\\]\n\np(x) = (2^(7/8) / gamma(1/4)) * exp(-2*x^4)\nfunction question3c()\n    x = LinRange(-2, 2, 1000)\n    fig = Figure()\n    ax = Axis(fig[1, 1],\n              title = L\"$p^*(x)=\\frac{2^{\\frac{7}{8}}}{\\Gamma\\left(\\frac{1}{4}\\right)}e^{-2x^4}$\",\n              xlabel = L\"$x$\",\n              ylabel = L\"$p^*(x)$\")\n    lines!(ax, x, p.(x))\n    save(\"question3c.png\", fig)\nend\nquestion3c();\n\n\n\n\n\n\n\n\n\n\nFigure 4: The pdf\n\n\n\n\n\n\nWe can see that the pdf is non-gaussian. A gaussian distribution is of the form \\(f(x) = e^{-x^2}\\)."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-c-1",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-c-1",
    "title": "Homework 3",
    "section": "Part C",
    "text": "Part C\nWrite the conditional transition density \\(p(x_{k+1}|x_k)\\) defined by discrete Markov process (Equation 2). Does the functional form of the transition density depend on the particular time \\(t_k\\)? Or is it the same for all times?\n\nSolution\nWe know that the Wiener process is a gaussian random variable with zero mean and variance \\(\\Delta t\\). For equation (Equation 2) we have \\(Var(\\frac{1}{2} \\Delta W_k) = \\frac{1}{4}\\Delta t\\).\nBy the property that the sum of a constant and a gaussian random variable is still a gaussian with a shifted mean, we can see that if the current state \\(X_k = x\\), the next state \\(X_{k+1}\\) is\n\\[\\begin{align}\n    X_{k+1} = x - x^3 \\Delta t + \\frac{1}{2} \\Delta W_k\n\\end{align}\\]\nThus\n\\[\\begin{align}\n    X_{k+1} | X_k = x \\sim \\mathcal{N}(x-x^3 \\Delta t, \\frac{1}{4} \\Delta t)\n\\end{align}\\]\nFrom this we can explicitly write out the conditional transition density \\(p(x_{k+1} | x_k)\\). Let \\(X_{k+1} = y\\) and \\(X_k = x\\)\n\\[\\begin{align}\n    p(y | x) = \\frac{2}{\\sqrt{2 \\pi \\Delta t}}e^{-\\frac{2(y - x + x^3 \\Delta t)^2}{\\Delta t}}\n\\end{align}\\]\nThe functional form of the transition density only depends on \\(\\Delta t\\), which is constant for all time."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-d",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-d",
    "title": "Homework 3",
    "section": "Part D",
    "text": "Part D\nBy using numerical integration show that the PDF \\(p^*(x)\\) of the statistical steady state you computed in part b is a solution to the fixed point problem\n\\[p^*(x) = \\int_{-\\infty}^{\\infty} p(x|y)p^*(y)dy \\tag{3}\\]\nwhere \\(p(x|y)\\) is the transition density you computed in part c. Given that \\(p^*(y)\\) decays quite fast, for numerical purposes it is sufficient to approximate the infinite domain of the integral (Equation 3) to [-5, 5].\n\nSolution"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-e",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-e",
    "title": "Homework 3",
    "section": "Part E",
    "text": "Part E\nPlot a few sample paths of the SDE for \\(\\Delta = 10^{-4}\\) for \\(t \\in [0, 5]\\).\n\nSolution\n\nfunction question3partE()\n    Δt = 1e-4\n    ts = 0.0:Δt:5.0\n    N = 5\n    W = Normal(0, sqrt(Δt))\n    procs = Matrix{Float64}(undef, length(ts), N)\n    for i in 1:N\n        procs[1, i] = rand() + 1.0\n    end\n\n    for i in 1:N\n        for j in 2:length(ts)\n            procs[j, i] = procs[j-1, i] - (procs[j-1, i]^3)*Δt + 0.5*rand(W)\n        end\n    end\n\n    fig = Figure()\n    ax = Axis(fig[1, 1],\n        title = L\"$X_{k+1} = X_k - X_k^3 \\Delta t + \\frac{1}{2} \\Delta W_k$\",\n        xlabel = \"x\")\n    for i in 1:N\n        lines!(ax, ts, procs[:, i])\n    end\n    save(\"question3e.png\", fig)\nend\nquestion3partE();\n\n\n\n\n\n\n\n\n\n\nFigure 5: 5 sample paths of the SDE for \\(\\Delta = 10^{-4}\\) for \\(t \\in [0, 5]\\)"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw3/hw3.html#part-f",
    "href": "courses/uncertainty_quant/docs/hw3/hw3.html#part-f",
    "title": "Homework 3",
    "section": "Part F",
    "text": "Part F\nBy computing a sufficiently large number of sample paths, Estimate the PDF of \\(X(t;\\omega)\\) numerically (e.g. by using a kernel density PDF estimator or method of relative frequencies) at different times and show that it converges to the steady state PDF you computed in part b.\n\nSolution\n\nfunction question3partF()\n    Δt = 1e-4\n    ts = 0.0:Δt:5.0\n    N = 200\n    W = Normal(0, sqrt(Δt))\n    procs = Matrix{Float64}(undef, length(ts), N)\n    println(\"initializing matrix with uniform random numbers in [1, 2]\")\n    for i in 1:N\n        procs[1, i] = rand() + 1.0\n    end\n    println(\"simulating random process\")\n    for i in 1:N\n        println(\"$N complete\")\n        for j in 2:length(ts)\n            procs[j, i] = procs[j-1, i] - (procs[j-1, i]^3)*Δt + 0.5*rand(W)\n        end\n    end\n\n    fig = Figure();display(fig)\n    ax1 = Axis(fig[1, 1],\n    title = \"$N SDE paths\")\n    ax2 = Axis(fig[1, 2],\n        title = \"KDE Density\")\n    x = LinRange(-5, 5, 1000)\n    xlims!(ax2, -5, 5)\n    for i in 1:N\n        lines!(ax1, ts, procs[:, i], linewidth = 1)\n    end\n    d = kde(procs[1, :])\n    vlinet = Observable(ts[1])\n    kde_data = Observable((d.x, d.density))\n    kde_line = lines!(ax2, [0.0], [0.0], color = :blue, label = \"KDE\")\n\n    kde_plot = lift(kde_data) do (x, density)\n        kde_line[1] = x\n        kde_line[2] = density\n    end\n\n    vlines!(ax1, vlinet, color = :red, label = \"time\")\n    lines!(ax2, x, p.(x), color = :red, linestyle = :dash, label = L\"$p^*(x)$\")\n    Legend(fig[2, 1], ax1, orientation = :horizontal)\n    Legend(fig[2, 2], ax2, orientation = :horizontal)\n    println(\"starting video rendering...\")\n    record(fig, \"question3partF.mp4\", 2:100:length(ts); framerate = 30) do k\n        println(\"frame $k\")\n        vlinet[] = ts[k]\n        d = kde(procs[k, :])\n        kde_data[] = (d.x, d.density)\n    end\nend\nquestion3partF()\n\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw4/hw4.html",
    "href": "courses/uncertainty_quant/docs/hw4/hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Consider the system of SDEs \n\\[\\begin{cases}dX(t;\\omega) &= -X(t;\\omega)^3dt + dY(t;\\omega) \\\\dY(t;\\omega) &= -\\tau Y(t;\\omega)dt + \\sigma dW(t;\\omega)\\end{cases} \\tag{1}\\]\nwhere \\(\\sigma, \\tau \\leq 0\\) are given parameters and \\(W(t;\\omega)\\) is a Wiener process. The initial condition \\((X(0;\\omega), Y(0;\\omega))\\) has i.i.d. components both of which are uniformly distributed in \\([0, 1]\\), i.e., \\(X(0;\\omega)\\) and \\(Y(0;\\omega)\\) are independent random variables with uniform PDF in \\([0,1]\\).\n\n\nPlot a few sample paths of \\(X(t;\\omega)\\) for \\(\\sigma = 0.1\\) and \\(\\tau = \\{0.01, 1, 10\\}\\).\n\n\nWe convert Equation 1 into a numerical simulation by first converting the SDE into a discrete time form via the Euler-Maruyama discretization.\nWe descritize a grid within the interval [0, T] into N equal parts, where \\(\\Delta t = \\frac{T}{N}\\)\nLet us recursively define \\(Y_n\\) for \\(0 \\leq n \\leq N-1\\)\n\\[\\begin{align}\n    Y_{n+1} = Y_n  -\\tau Y_n \\Delta t + \\sigma \\Delta W\n\\end{align}\\]\nand \\(X_n\\)\n\\[\\begin{align}\n    X_{n+1} = X_n - X_n^3 \\Delta t + \\Delta Y_n\n\\end{align}\\]\nwhere \\(\\Delta Y_n = Y_{n+1} - Y_n = -\\tau Y_n \\Delta t + \\sigma \\Delta W_n\\)\nwhere \\(\\Delta W_n\\) are i.i.d Gaussian random variables with zero mean and variance \\(\\Delta t\\).\n\nusing GLMakie\nusing Distributions\n\nfunction partA(τ::Float64, σ::Float64)\n    # define the length of subintervals\n    Δt = 1e-4\n    ts = 0.0:Δt:5.0\n    # number of samples\n    N = 5\n    # Weiner processs\n    W = Normal(0, sqrt(Δt))\n    # initialize mesh\n    X = Matrix{Float64}(undef, length(ts), N)\n    Y = Matrix{Float64}(undef, length(ts), N)\n\n    # apply initial conditions\n    for i in 1:N\n        X[1, i] = rand()\n        Y[1, i] = rand()\n    end\n    # propagate the process\n    for i in 1:N\n        for j in 2:length(ts)\n            ΔW = rand(W)\n            X[j, i] = X[j-1, i] - (X[j-1, i]^3)*Δt -τ*Y[j-1, i]*Δt + σ*ΔW\n            Y[j, i] = Y[j-1, i] - τ*Y[j-1, i]*Δt + σ*ΔW\n        end\n    end\n    # make the figure\n    fig = Figure()\n    ax = Axis(\n        fig[1, 1],\n        title = \"σ = $σ τ = $τ\",\n        xlabel = \"t\",\n        ylabel = L\"$X_{n+1}$\"\n    )\n    # set y axis limits\n    ylims!(ax, -1.0, 1.0)\n\n    # Plot the samples\n    for i in 1:N\n        lines!(ax, ts, X[:, i])\n    end\n    return fig\nend\nsave(\"parta01.png\", partA(0.01, 0.1))\nsave(\"parta1.png\", partA(1.0, 0.1))\nsave(\"parta10.png\", partA(10.0, 0.1))\n\n\n\n\n\n\n\\(\\tau = 0.01\\)\n\n\n\n\n\n\n\\(\\tau = 1.0\\)\n\n\n\n\n\n\n\\(\\tau = 10.0\\)\n\n\n\n\n\n\n\n\nDo you expect the system Equation 1 to have a statistically stationary state? Justify your answer.\n\n\nFrom part A, we can see that as we increase \\(\\tau\\), the system rapidly converges to a statistically steady-state process. We can prove this by the Kernel Density Estimation for 1000 sample paths for varying \\(\\tau\\) and comparing the estimated PDF for later times.\nThe solution approaches a statistically steady state as \\(t\\rightarrow \\infty\\) and approaches the statisically steady state at the rate \\(\\tau\\). It seems though that the pdf for varying \\(\\tau\\) may not be the same, as \\(\\tau = 10.0\\) seems to have a bimodal distribution for later times.\n\nusing KernelDensity\n\nfunction simulateB(τ::Float64)\n    σ = 0.1\n    # define the length of subintervals\n    Δt = 1e-4\n    ts = 0.0:Δt:20.0\n    # number of samples\n    N = 1000\n    # Weiner processs\n    W = Normal(0, sqrt(Δt))\n    # initialize mesh\n    X = Matrix{Float64}(undef, length(ts), N)\n    Y = Matrix{Float64}(undef, length(ts), N)\n\n    # apply initial conditions\n    for i in 1:N\n        X[1, i] = rand()\n        Y[1, i] = rand()\n    end\n    # propagate the process\n    for i in 1:N\n        for j in 2:length(ts)\n            ΔW = rand(W)\n            X[j, i] = X[j-1, i] - (X[j-1, i]^3)*Δt -τ*Y[j-1, i]*Δt + σ*ΔW\n            Y[j, i] = Y[j-1, i] - τ*Y[j-1, i]*Δt + σ*ΔW\n        end\n    end\n    return X, Y, length(ts)\nend\n\nfunction partB()\n    X1, Y1, ts = simulateB(0.01)\n    X2, Y2, _ = simulateB(1.0)\n    X3, Y3, _ = simulateB(10.0)\n\n    fig = Figure()\n    ax = Axis(\n        fig[1, 1]\n    )\n    ylims!(ax, 0.0, 3.0)\n    xlims!(ax, -2.0, 2.0)\n\n    d1 = kde(X1[1, :])\n    d2 = kde(X2[1, :])\n    d3 = kde(X3[1, :])\n    kde_data1 = Observable((d1.x, d1.density))\n    kde_data2 = Observable((d2.x, d2.density))\n    kde_data3 = Observable((d3.x, d3.density))\n\n    kde_line1 = lines!(ax, [0.0], [0.0], color = :red, label = \"τ = 0.01\")\n    kde_line2 = lines!(ax, [0.0], [0.0], color = :blue, label = \"τ = 1.0\")\n    kde_line3 = lines!(ax, [0.0], [0.0], color = :green, label = \"τ = 10.0\")\n\n    kde_plot1 = lift(kde_data1) do (x, density)\n        kde_line1[1] = x\n        kde_line1[2] = density\n    end\n    kde_plot2 = lift(kde_data2) do (x, density)\n        kde_line2[1] = x\n        kde_line2[2] = density\n    end\n    kde_plot3 = lift(kde_data3) do (x, density)\n        kde_line3[1] = x\n        kde_line3[2] = density\n    end\n\n    Legend(fig[1, 2], ax)\n    record(fig, \"partb.mp4\", 2:400:ts; framerate = 30) do k\n        d1 = kde(X1[k, :])\n        d2 = kde(X2[k, :])\n        d3 = kde(X3[k, :])\n        kde_data1[] = (d1.x, d1.density)\n        kde_data2[] = (d2.x, d2.density)\n        kde_data3[] = (d3.x, d3.density)\n    end\n    return fig\nend\npartB()\n\n\n\n\n\n\n\n\n\n\n\nWrite the Fokker-Planck equation for the joint PDF of \\(X(t;\\omega)\\) and \\(Y(t;\\omega)\\).\n\n\nLet us write Equation 1 in state-space form\n\\[\\begin{align}\n    dX &= -X^3 dt - \\tau Y dt + \\sigma dW \\\\\n    dY &= -\\tau Y dt + \\sigma dW\n\\end{align}\\]\nThe vector \\(\\mathbf{G}(\\mathbf{X}, t)\\) can be written as\n\\[\\begin{align}\n     \\mathbf{G}(\\mathbf{X}, t) &= \\left[\n     \\begin{matrix}\n         -X^3 - \\tau Y \\\\\n         -\\tau Y\n     \\end{matrix}\\right]\n\\end{align}\\]\nThe matrix \\(\\mathbf{S}\\) can be written as\n\\[\\begin{align}\n    \\mathbf{S} = \\left[\n    \\begin{matrix}\n        \\sigma \\\\\n        \\sigma\n    \\end{matrix}\\right]\n\\end{align}\\]\nWe can thus write the Fokker-Planck equation according equation 59 in course notes 4 as follows:\n\\[\\begin{align}\n    \\frac{\\partial p(\\mathbf{x}, t)}{\\partial t} &= -\\sum_{k=1}^{2} \\frac{\\partial}{\\partial x_k} \\left(G_k(\\mathbf{x}, t) p(\\mathbf{x})\\right) + \\frac{1}{2} \\sum_{i, k = 1}^2 \\frac{\\partial^2}{\\partial x_i \\partial x_k}\\left(\\sum_{j=1}^2 S_{ij}(\\mathbf{x}, t)S_{kj}(\\mathbf{x}, t) p(\\mathbf{x}, t) \\right) \\\\\n\\frac{\\partial p}{\\partial t} &= -\\left(\\frac{\\partial}{\\partial x} \\left((-x^3 -\\tau y)p\\right) + \\frac{\\partial}{\\partial y}\\left(-\\tau yp\\right)\\right) + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2p}{\\partial x^2} + \\frac{\\partial^2 p}{\\partial y^2}\\right) \\\\\n&= \\frac{\\partial}{\\partial x}(x^3 p) + \\tau p + \\tau y\\left(\\frac{\\partial p}{\\partial x} + \\frac{\\partial p}{\\partial y}\\right) + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2p}{\\partial x^2} + \\frac{\\partial^2 p}{\\partial y^2}\\right)\n\\end{align}\\]\nThe Fokker-Planck equation is thus\n\\[\\frac{\\partial p(x, y, t)}{\\partial t} = \\frac{\\partial}{\\partial x}(x^3 p(x, y, t)) + \\tau p(x, y, t) + \\tau y\\left(\\frac{\\partial p(x, y, t)}{\\partial x} + \\frac{\\partial p(x, y, t)}{\\partial y}\\right) + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2p(x, y, t)}{\\partial x^2} + \\frac{\\partial^2 p(x, y, t)}{\\partial y^2}\\right) \\tag{2}\\]\nwhere \\(p(x, y, t)\\) is the joint PDF of \\(X\\) and \\(Y\\).\n\n\n\n\nWrite the reduced-order equation for the joint PDF of \\(X(t;\\omega)\\) in terms of the conditional expectation \\(\\mathbb{E}\\{Y(t;\\omega)|X(t;\\omega)\\}\\).\n\n\nTo obtain the reduced-order equation, let us integrate Equation 2 with respect to \\(y\\) and use the definition of conditional PDF.\n\\[\\begin{align}\n\\frac{\\partial}{\\partial t}\\int_{-\\infty}^{\\infty}pdy &= \\int_{-\\infty}^{\\infty}\\left(\\frac{\\partial}{\\partial x} \\left((x^3 +\\tau y)p\\right) + \\frac{\\partial}{\\partial y}\\left(\\tau yp\\right) + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2p}{\\partial x^2} + \\frac{\\partial^2 p}{\\partial y^2}\\right)\\right) dy\\\\\n\\frac{\\partial p(x,t)}{\\partial t} &= \\frac{\\partial}{\\partial x} \\int_{-\\infty}^{\\infty}(x^3 + \\tau y)p dy + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial y}(\\tau yp)dy + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2}{\\partial x^2}\\int_{-\\infty}^{\\infty}p dy + \\cancelto{0}{\\int_{-\\infty}^{\\infty}\\frac{\\partial^2 p}{\\partial y^2}dy}\\right) \\\\\n&= \\frac{\\partial}{\\partial x}\\left(x^3p(x, t) + \\tau p(x, t)\\mathbb{E}\\{Y | X\\}\\right) + \\cancelto{0}{\\tau y p_{\\bigg{|}_{-\\infty}^{\\infty}}} + \\frac{\\sigma^2}{2}\\frac{\\partial^2 p(x, t)}{\\partial x^2}\\\\\n\\end{align}\\]\nThus, the reduced order equation for the joint PDF can be written as,\n\\[\\frac{\\partial p(x, t)}{\\partial t} = \\frac{\\partial}{\\partial x}(x^3 p(x, t)) + \\tau \\frac{\\partial}{\\partial x}\\left(p(x, t)\\mathbb{E}\\{y|x\\}\\right) + \\frac{\\sigma^2}{2} \\frac{\\partial^2 p(x, t)}{\\partial x^2} \\tag{3}\\]\n\n\n\n\nSet \\(\\sigma = 0\\). Compute the conditional expectation \\(\\mathbb{E}\\{Y(t;\\omega) | X(t;\\omega)\\}\\) explicity as a function of \\(t\\) and substitute it in the reduced order equation you obtained in part d (with \\(\\sigma = 0\\)) to obtain an exact (and closed) equation for the PDF of \\(X(t;\\omega)\\).\n\n\nRecall the orginal SDE Equation 1 where\n\\[\\begin{align}\n    dY = -\\tau Y dt + \\sigma dW\n\\end{align}\\]\nsetting \\(\\sigma = 0\\) we have the following ODE:\n\\[\\begin{align}\n    \\begin{cases}\n        \\frac{dY}{dt} = -\\tau Y \\\\\n        Y(0) \\sim \\text{Uniform}(0, 1)\n    \\end{cases}\n\\end{align}\\]\nsolving for \\(Y(t)\\) we have:\n\\[\\begin{align}\n    Y(t) = Y(0)e^{-\\tau t}\n\\end{align}\\]\nwhere \\(Y(0)\\) can be expressed as the expectation of a uniform random variable in [0, 1], which we know is \\(\\frac{1}{2}\\)\ntherefore the conditional expectation \\[\\mathbb{E}\\{Y|X\\} = \\mathbb{E}\\{Y\\} = \\frac{1}{2}e^{-\\tau t}\\]\nwe can write a closed equation for the reduced order equation as follows:\n\\[\\frac{\\partial p(x, t)}{\\partial t} = \\frac{\\partial }{\\partial x}(x^3p(x, t)) + \\tau \\frac{\\partial }{\\partial x}(p(x, t)\\frac{1}{2}e^{-\\tau t}) \\tag{4}\\]\n\n\n\n\nWrite the PDF equation you obtained in part e as an evolution equation for the cumulative distribution function (CDF) of \\(X(t;\\omega)\\).\n\n\nRecall the definition of the cumulative distribution function:\n\\[\\begin{align}\nF(x, y, t) = \\int_{-\\infty}^{x}p(y, t)dy\n\\end{align}\\]\nif we take the derivative on both sides with respect to \\(x\\) we have\n\\[\\begin{align}\np(x, t) = \\frac{\\partial F(x, t)}{\\partial x}\n\\end{align}\\]\nplugging this into Equation 4 we have\n\\[\\begin{align}\n\\frac{\\partial }{\\partial t}\\left(\\frac{\\partial F}{\\partial x}\\right) = \\frac{\\partial }{\\partial x}\\left(x^3\\frac{\\partial F}{\\partial x}\\right) + \\tau \\frac{\\partial }{\\partial x}\\left(\\frac{\\partial F}{\\partial x}\\frac{1}{2}e^{-\\tau t}\\right)\n\\end{align}\\]\ntaking the integral of both sides:\n\\[\\begin{align}\n    \\int_{-\\infty}^{x}\\frac{\\partial^2 F}{\\partial t\\partial x'}dx' &= \\int_{-\\infty}^{x}\\left(\\frac{\\partial }{\\partial x'}\\left(x'^3\\frac{\\partial F}{\\partial x'}\\right) + \\frac{\\tau}{2} \\frac{\\partial }{\\partial x'}\\left(\\frac{\\partial F}{\\partial x'}e^{-\\tau t}\\right)\\right)dx' \\\\\n\\frac{\\partial F(x, t)}{\\partial t} &= x'^3 \\frac{\\partial F}{\\partial x'}\\bigg|_{-\\infty}^{x} + \\frac{\\tau}{2} \\frac{\\partial F}{\\partial x'}e^{-\\tau t}\\bigg|_{-\\infty}^{x} \\\\\n\\end{align}\\]\nthus we are left with the evolution equation for \\(F(x, t)\\)\n\\[\\frac{\\partial F(x, t)}{\\partial t} = x^3 \\frac{\\partial F(x, t)}{\\partial x} + \\frac{\\tau}{2}e^{-\\tau t}\\frac{\\partial F(x, t)}{\\partial x}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw4/hw4.html#part-a",
    "href": "courses/uncertainty_quant/docs/hw4/hw4.html#part-a",
    "title": "Homework 4",
    "section": "",
    "text": "Plot a few sample paths of \\(X(t;\\omega)\\) for \\(\\sigma = 0.1\\) and \\(\\tau = \\{0.01, 1, 10\\}\\).\n\n\nWe convert Equation 1 into a numerical simulation by first converting the SDE into a discrete time form via the Euler-Maruyama discretization.\nWe descritize a grid within the interval [0, T] into N equal parts, where \\(\\Delta t = \\frac{T}{N}\\)\nLet us recursively define \\(Y_n\\) for \\(0 \\leq n \\leq N-1\\)\n\\[\\begin{align}\n    Y_{n+1} = Y_n  -\\tau Y_n \\Delta t + \\sigma \\Delta W\n\\end{align}\\]\nand \\(X_n\\)\n\\[\\begin{align}\n    X_{n+1} = X_n - X_n^3 \\Delta t + \\Delta Y_n\n\\end{align}\\]\nwhere \\(\\Delta Y_n = Y_{n+1} - Y_n = -\\tau Y_n \\Delta t + \\sigma \\Delta W_n\\)\nwhere \\(\\Delta W_n\\) are i.i.d Gaussian random variables with zero mean and variance \\(\\Delta t\\).\n\nusing GLMakie\nusing Distributions\n\nfunction partA(τ::Float64, σ::Float64)\n    # define the length of subintervals\n    Δt = 1e-4\n    ts = 0.0:Δt:5.0\n    # number of samples\n    N = 5\n    # Weiner processs\n    W = Normal(0, sqrt(Δt))\n    # initialize mesh\n    X = Matrix{Float64}(undef, length(ts), N)\n    Y = Matrix{Float64}(undef, length(ts), N)\n\n    # apply initial conditions\n    for i in 1:N\n        X[1, i] = rand()\n        Y[1, i] = rand()\n    end\n    # propagate the process\n    for i in 1:N\n        for j in 2:length(ts)\n            ΔW = rand(W)\n            X[j, i] = X[j-1, i] - (X[j-1, i]^3)*Δt -τ*Y[j-1, i]*Δt + σ*ΔW\n            Y[j, i] = Y[j-1, i] - τ*Y[j-1, i]*Δt + σ*ΔW\n        end\n    end\n    # make the figure\n    fig = Figure()\n    ax = Axis(\n        fig[1, 1],\n        title = \"σ = $σ τ = $τ\",\n        xlabel = \"t\",\n        ylabel = L\"$X_{n+1}$\"\n    )\n    # set y axis limits\n    ylims!(ax, -1.0, 1.0)\n\n    # Plot the samples\n    for i in 1:N\n        lines!(ax, ts, X[:, i])\n    end\n    return fig\nend\nsave(\"parta01.png\", partA(0.01, 0.1))\nsave(\"parta1.png\", partA(1.0, 0.1))\nsave(\"parta10.png\", partA(10.0, 0.1))\n\n\n\n\n\n\n\\(\\tau = 0.01\\)\n\n\n\n\n\n\n\\(\\tau = 1.0\\)\n\n\n\n\n\n\n\\(\\tau = 10.0\\)"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw4/hw4.html#part-b",
    "href": "courses/uncertainty_quant/docs/hw4/hw4.html#part-b",
    "title": "Homework 4",
    "section": "",
    "text": "Do you expect the system Equation 1 to have a statistically stationary state? Justify your answer.\n\n\nFrom part A, we can see that as we increase \\(\\tau\\), the system rapidly converges to a statistically steady-state process. We can prove this by the Kernel Density Estimation for 1000 sample paths for varying \\(\\tau\\) and comparing the estimated PDF for later times.\nThe solution approaches a statistically steady state as \\(t\\rightarrow \\infty\\) and approaches the statisically steady state at the rate \\(\\tau\\). It seems though that the pdf for varying \\(\\tau\\) may not be the same, as \\(\\tau = 10.0\\) seems to have a bimodal distribution for later times.\n\nusing KernelDensity\n\nfunction simulateB(τ::Float64)\n    σ = 0.1\n    # define the length of subintervals\n    Δt = 1e-4\n    ts = 0.0:Δt:20.0\n    # number of samples\n    N = 1000\n    # Weiner processs\n    W = Normal(0, sqrt(Δt))\n    # initialize mesh\n    X = Matrix{Float64}(undef, length(ts), N)\n    Y = Matrix{Float64}(undef, length(ts), N)\n\n    # apply initial conditions\n    for i in 1:N\n        X[1, i] = rand()\n        Y[1, i] = rand()\n    end\n    # propagate the process\n    for i in 1:N\n        for j in 2:length(ts)\n            ΔW = rand(W)\n            X[j, i] = X[j-1, i] - (X[j-1, i]^3)*Δt -τ*Y[j-1, i]*Δt + σ*ΔW\n            Y[j, i] = Y[j-1, i] - τ*Y[j-1, i]*Δt + σ*ΔW\n        end\n    end\n    return X, Y, length(ts)\nend\n\nfunction partB()\n    X1, Y1, ts = simulateB(0.01)\n    X2, Y2, _ = simulateB(1.0)\n    X3, Y3, _ = simulateB(10.0)\n\n    fig = Figure()\n    ax = Axis(\n        fig[1, 1]\n    )\n    ylims!(ax, 0.0, 3.0)\n    xlims!(ax, -2.0, 2.0)\n\n    d1 = kde(X1[1, :])\n    d2 = kde(X2[1, :])\n    d3 = kde(X3[1, :])\n    kde_data1 = Observable((d1.x, d1.density))\n    kde_data2 = Observable((d2.x, d2.density))\n    kde_data3 = Observable((d3.x, d3.density))\n\n    kde_line1 = lines!(ax, [0.0], [0.0], color = :red, label = \"τ = 0.01\")\n    kde_line2 = lines!(ax, [0.0], [0.0], color = :blue, label = \"τ = 1.0\")\n    kde_line3 = lines!(ax, [0.0], [0.0], color = :green, label = \"τ = 10.0\")\n\n    kde_plot1 = lift(kde_data1) do (x, density)\n        kde_line1[1] = x\n        kde_line1[2] = density\n    end\n    kde_plot2 = lift(kde_data2) do (x, density)\n        kde_line2[1] = x\n        kde_line2[2] = density\n    end\n    kde_plot3 = lift(kde_data3) do (x, density)\n        kde_line3[1] = x\n        kde_line3[2] = density\n    end\n\n    Legend(fig[1, 2], ax)\n    record(fig, \"partb.mp4\", 2:400:ts; framerate = 30) do k\n        d1 = kde(X1[k, :])\n        d2 = kde(X2[k, :])\n        d3 = kde(X3[k, :])\n        kde_data1[] = (d1.x, d1.density)\n        kde_data2[] = (d2.x, d2.density)\n        kde_data3[] = (d3.x, d3.density)\n    end\n    return fig\nend\npartB()"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw4/hw4.html#part-c",
    "href": "courses/uncertainty_quant/docs/hw4/hw4.html#part-c",
    "title": "Homework 4",
    "section": "",
    "text": "Write the Fokker-Planck equation for the joint PDF of \\(X(t;\\omega)\\) and \\(Y(t;\\omega)\\).\n\n\nLet us write Equation 1 in state-space form\n\\[\\begin{align}\n    dX &= -X^3 dt - \\tau Y dt + \\sigma dW \\\\\n    dY &= -\\tau Y dt + \\sigma dW\n\\end{align}\\]\nThe vector \\(\\mathbf{G}(\\mathbf{X}, t)\\) can be written as\n\\[\\begin{align}\n     \\mathbf{G}(\\mathbf{X}, t) &= \\left[\n     \\begin{matrix}\n         -X^3 - \\tau Y \\\\\n         -\\tau Y\n     \\end{matrix}\\right]\n\\end{align}\\]\nThe matrix \\(\\mathbf{S}\\) can be written as\n\\[\\begin{align}\n    \\mathbf{S} = \\left[\n    \\begin{matrix}\n        \\sigma \\\\\n        \\sigma\n    \\end{matrix}\\right]\n\\end{align}\\]\nWe can thus write the Fokker-Planck equation according equation 59 in course notes 4 as follows:\n\\[\\begin{align}\n    \\frac{\\partial p(\\mathbf{x}, t)}{\\partial t} &= -\\sum_{k=1}^{2} \\frac{\\partial}{\\partial x_k} \\left(G_k(\\mathbf{x}, t) p(\\mathbf{x})\\right) + \\frac{1}{2} \\sum_{i, k = 1}^2 \\frac{\\partial^2}{\\partial x_i \\partial x_k}\\left(\\sum_{j=1}^2 S_{ij}(\\mathbf{x}, t)S_{kj}(\\mathbf{x}, t) p(\\mathbf{x}, t) \\right) \\\\\n\\frac{\\partial p}{\\partial t} &= -\\left(\\frac{\\partial}{\\partial x} \\left((-x^3 -\\tau y)p\\right) + \\frac{\\partial}{\\partial y}\\left(-\\tau yp\\right)\\right) + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2p}{\\partial x^2} + \\frac{\\partial^2 p}{\\partial y^2}\\right) \\\\\n&= \\frac{\\partial}{\\partial x}(x^3 p) + \\tau p + \\tau y\\left(\\frac{\\partial p}{\\partial x} + \\frac{\\partial p}{\\partial y}\\right) + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2p}{\\partial x^2} + \\frac{\\partial^2 p}{\\partial y^2}\\right)\n\\end{align}\\]\nThe Fokker-Planck equation is thus\n\\[\\frac{\\partial p(x, y, t)}{\\partial t} = \\frac{\\partial}{\\partial x}(x^3 p(x, y, t)) + \\tau p(x, y, t) + \\tau y\\left(\\frac{\\partial p(x, y, t)}{\\partial x} + \\frac{\\partial p(x, y, t)}{\\partial y}\\right) + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2p(x, y, t)}{\\partial x^2} + \\frac{\\partial^2 p(x, y, t)}{\\partial y^2}\\right) \\tag{2}\\]\nwhere \\(p(x, y, t)\\) is the joint PDF of \\(X\\) and \\(Y\\)."
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw4/hw4.html#part-d",
    "href": "courses/uncertainty_quant/docs/hw4/hw4.html#part-d",
    "title": "Homework 4",
    "section": "",
    "text": "Write the reduced-order equation for the joint PDF of \\(X(t;\\omega)\\) in terms of the conditional expectation \\(\\mathbb{E}\\{Y(t;\\omega)|X(t;\\omega)\\}\\).\n\n\nTo obtain the reduced-order equation, let us integrate Equation 2 with respect to \\(y\\) and use the definition of conditional PDF.\n\\[\\begin{align}\n\\frac{\\partial}{\\partial t}\\int_{-\\infty}^{\\infty}pdy &= \\int_{-\\infty}^{\\infty}\\left(\\frac{\\partial}{\\partial x} \\left((x^3 +\\tau y)p\\right) + \\frac{\\partial}{\\partial y}\\left(\\tau yp\\right) + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2p}{\\partial x^2} + \\frac{\\partial^2 p}{\\partial y^2}\\right)\\right) dy\\\\\n\\frac{\\partial p(x,t)}{\\partial t} &= \\frac{\\partial}{\\partial x} \\int_{-\\infty}^{\\infty}(x^3 + \\tau y)p dy + \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial y}(\\tau yp)dy + \\frac{\\sigma^2}{2}\\left(\\frac{\\partial^2}{\\partial x^2}\\int_{-\\infty}^{\\infty}p dy + \\cancelto{0}{\\int_{-\\infty}^{\\infty}\\frac{\\partial^2 p}{\\partial y^2}dy}\\right) \\\\\n&= \\frac{\\partial}{\\partial x}\\left(x^3p(x, t) + \\tau p(x, t)\\mathbb{E}\\{Y | X\\}\\right) + \\cancelto{0}{\\tau y p_{\\bigg{|}_{-\\infty}^{\\infty}}} + \\frac{\\sigma^2}{2}\\frac{\\partial^2 p(x, t)}{\\partial x^2}\\\\\n\\end{align}\\]\nThus, the reduced order equation for the joint PDF can be written as,\n\\[\\frac{\\partial p(x, t)}{\\partial t} = \\frac{\\partial}{\\partial x}(x^3 p(x, t)) + \\tau \\frac{\\partial}{\\partial x}\\left(p(x, t)\\mathbb{E}\\{y|x\\}\\right) + \\frac{\\sigma^2}{2} \\frac{\\partial^2 p(x, t)}{\\partial x^2} \\tag{3}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw4/hw4.html#part-e",
    "href": "courses/uncertainty_quant/docs/hw4/hw4.html#part-e",
    "title": "Homework 4",
    "section": "",
    "text": "Set \\(\\sigma = 0\\). Compute the conditional expectation \\(\\mathbb{E}\\{Y(t;\\omega) | X(t;\\omega)\\}\\) explicity as a function of \\(t\\) and substitute it in the reduced order equation you obtained in part d (with \\(\\sigma = 0\\)) to obtain an exact (and closed) equation for the PDF of \\(X(t;\\omega)\\).\n\n\nRecall the orginal SDE Equation 1 where\n\\[\\begin{align}\n    dY = -\\tau Y dt + \\sigma dW\n\\end{align}\\]\nsetting \\(\\sigma = 0\\) we have the following ODE:\n\\[\\begin{align}\n    \\begin{cases}\n        \\frac{dY}{dt} = -\\tau Y \\\\\n        Y(0) \\sim \\text{Uniform}(0, 1)\n    \\end{cases}\n\\end{align}\\]\nsolving for \\(Y(t)\\) we have:\n\\[\\begin{align}\n    Y(t) = Y(0)e^{-\\tau t}\n\\end{align}\\]\nwhere \\(Y(0)\\) can be expressed as the expectation of a uniform random variable in [0, 1], which we know is \\(\\frac{1}{2}\\)\ntherefore the conditional expectation \\[\\mathbb{E}\\{Y|X\\} = \\mathbb{E}\\{Y\\} = \\frac{1}{2}e^{-\\tau t}\\]\nwe can write a closed equation for the reduced order equation as follows:\n\\[\\frac{\\partial p(x, t)}{\\partial t} = \\frac{\\partial }{\\partial x}(x^3p(x, t)) + \\tau \\frac{\\partial }{\\partial x}(p(x, t)\\frac{1}{2}e^{-\\tau t}) \\tag{4}\\]"
  },
  {
    "objectID": "courses/uncertainty_quant/docs/hw4/hw4.html#part-f",
    "href": "courses/uncertainty_quant/docs/hw4/hw4.html#part-f",
    "title": "Homework 4",
    "section": "",
    "text": "Write the PDF equation you obtained in part e as an evolution equation for the cumulative distribution function (CDF) of \\(X(t;\\omega)\\).\n\n\nRecall the definition of the cumulative distribution function:\n\\[\\begin{align}\nF(x, y, t) = \\int_{-\\infty}^{x}p(y, t)dy\n\\end{align}\\]\nif we take the derivative on both sides with respect to \\(x\\) we have\n\\[\\begin{align}\np(x, t) = \\frac{\\partial F(x, t)}{\\partial x}\n\\end{align}\\]\nplugging this into Equation 4 we have\n\\[\\begin{align}\n\\frac{\\partial }{\\partial t}\\left(\\frac{\\partial F}{\\partial x}\\right) = \\frac{\\partial }{\\partial x}\\left(x^3\\frac{\\partial F}{\\partial x}\\right) + \\tau \\frac{\\partial }{\\partial x}\\left(\\frac{\\partial F}{\\partial x}\\frac{1}{2}e^{-\\tau t}\\right)\n\\end{align}\\]\ntaking the integral of both sides:\n\\[\\begin{align}\n    \\int_{-\\infty}^{x}\\frac{\\partial^2 F}{\\partial t\\partial x'}dx' &= \\int_{-\\infty}^{x}\\left(\\frac{\\partial }{\\partial x'}\\left(x'^3\\frac{\\partial F}{\\partial x'}\\right) + \\frac{\\tau}{2} \\frac{\\partial }{\\partial x'}\\left(\\frac{\\partial F}{\\partial x'}e^{-\\tau t}\\right)\\right)dx' \\\\\n\\frac{\\partial F(x, t)}{\\partial t} &= x'^3 \\frac{\\partial F}{\\partial x'}\\bigg|_{-\\infty}^{x} + \\frac{\\tau}{2} \\frac{\\partial F}{\\partial x'}e^{-\\tau t}\\bigg|_{-\\infty}^{x} \\\\\n\\end{align}\\]\nthus we are left with the evolution equation for \\(F(x, t)\\)\n\\[\\frac{\\partial F(x, t)}{\\partial t} = x^3 \\frac{\\partial F(x, t)}{\\partial x} + \\frac{\\tau}{2}e^{-\\tau t}\\frac{\\partial F(x, t)}{\\partial x}\\]"
  },
  {
    "objectID": "courses/adv_methods/index.html",
    "href": "courses/adv_methods/index.html",
    "title": "Advanced Methods in Applied Mathematics",
    "section": "",
    "text": "This course provides students with an overview of advanced topics and analytical techniques in Applied Mathematics, focusing on the analytical solution of ODEs and PDEs, including asymptotic analysis (method of strained coordinates; method of multiple scales; matched asymptotics; WKB theory), dimensional analysis, self-similar solutions, and Green’s function solutions of PDEs. It also introduces students to variational calculus for the optimization of functionals.\nInstructor: Pascale Garaud\n\n\n\nAM212 Lecture Notes"
  },
  {
    "objectID": "courses/adv_methods/index.html#course-description",
    "href": "courses/adv_methods/index.html#course-description",
    "title": "Advanced Methods in Applied Mathematics",
    "section": "",
    "text": "This course provides students with an overview of advanced topics and analytical techniques in Applied Mathematics, focusing on the analytical solution of ODEs and PDEs, including asymptotic analysis (method of strained coordinates; method of multiple scales; matched asymptotics; WKB theory), dimensional analysis, self-similar solutions, and Green’s function solutions of PDEs. It also introduces students to variational calculus for the optimization of functionals.\nInstructor: Pascale Garaud"
  },
  {
    "objectID": "courses/adv_methods/index.html#course-notes",
    "href": "courses/adv_methods/index.html#course-notes",
    "title": "Advanced Methods in Applied Mathematics",
    "section": "",
    "text": "AM212 Lecture Notes"
  },
  {
    "objectID": "projecteuler/multiplesof3or5.html",
    "href": "projecteuler/multiplesof3or5.html",
    "title": "Problem",
    "section": "",
    "text": "If we list all the natural numbers below \\(10\\) that are multiplies of \\(3\\) or \\(5\\), we get \\(3, 5, 6, \\text{and } 9\\). The sum of these multiples is \\(23\\).\nFind the sum of all the multiples of \\(3\\) or \\(5\\) below \\(1000\\).\n\n\n\n\nLet \\(A \\in \\mathbb{N}\\) be the set that contains all the natural numbers in the domain \\([1, 1000)\\) that are multiples of \\(3\\).\n\\[\\begin{align}\nA = 3, 6, 9, 12, 15, \\cdots, 999\n\\end{align}\\]\nLet \\(B \\in \\mathbb{N}\\) be the set that contains the numbers in the same domain as \\(A\\) but that are multiples of \\(5\\).\n\\[\\begin{align}\nB = 5, 10, 15, 20, 25, \\cdots, 995\n\\end{align}\\]\nEssentially what we are looking for here is the sum of all the elements contained in \\(A \\cup B\\).\nBy the inclusion-exculsion principle, recall that\n\\[\\begin{align}\nA \\cup B = A + B - A \\cap B\n\\end{align}\\]\nso we need to find the sum of the elements contained in \\(A\\), \\(B\\), and the set \\(C = A \\cap B\\) which is the set of all the numbers which are multiples of \\(15\\).\n\\[\\begin{align}\nC = 15, 30, 45, 60, \\cdots, 990\n\\end{align}\\]\nFor this we can use the formulas for Arithmetic Progression.\n\\[\\begin{align}\n\\sum A = n \\left(\\frac{a_1 + a_n}{2}\\right)\n\\end{align}\\]\nwhere \\(n\\) is the number of terms in the sequence, \\(a_1\\) is the first term and \\(a_n\\) is the last term. We don’t know what how many terms there are in the sequence so we can also use the nth term formula\n\\[\\begin{align}\na_n = a_1 + \\left(n - 1\\right)d\n\\end{align}\\]\nwhere \\(d\\) is the value being added to \\(a_{n-1}\\) to obtain \\(a_n\\).\nLet us compute these sums for \\(A, B, C\\)\n\n\nthe last term in the sequence is \\(999\\) thus,\n\\[\\begin{align}\n999 &= 3 + \\left(n - 1\\right)3\\\\\n999 &= 3n\\\\\nn &= 333\n\\end{align}\\]\nhence,\n\\[\\begin{align}\n\\sum A &= 333 \\left(\\frac{3 + 999}{2}\\right)\\\\\n&= 333 \\cdot \\frac{1002}{2} \\\\\n&= 166833\n\\end{align}\\]\n\n\n\nthe last term in the sequence is \\(995\\) thus,\n\\[\\begin{align}\n995 &= 5 + \\left(n - 1\\right)5\\\\\n995 &= 5n\\\\\nn &= 199\n\\end{align}\\]\nhence,\n\\[\\begin{align}\n\\sum B &= 199 \\left(\\frac{5 + 995}{2}\\right)\\\\\n&= 99500\n\\end{align}\\]\n\n\n\nthe last term in the sequence is \\(990\\) thus,\n\\[\\begin{align}\n990 &= 15 + \\left(n - 1\\right)15\\\\\n&= 15n\\\\\nn &= 66\n\\end{align}\\]\nhence,\n\\[\\begin{align}\n\\sum C &= 66 \\left(\\frac{15 + 990}{2}\\right)\\\\\n&= 66 \\cdot \\left(\\frac{1005}{2}\\right)\\\\\n&= 33165\n\\end{align}\\]\nTherefore the final solution,\n\\[\\begin{align}\n\\sum A \\cup B = 166833 + 99500 - 33165 = 233168\n\\end{align}\\]\n\n\n\n\nWe can solve the problem brute force by writing a code that computes the sum of any number in the range \\([1, 1000]\\) that is a multiple of \\(3\\) or \\(5\\).\n\nsum(x for x in 1:999 if (x % 3 == 0) || (x % 5 == 0))\n\n233168\n\n\nWe use a generator function in the call to sum so that we don’t generate an intermediate array and then sum over the elements of the array. Instead it simply keeps a running total of the elements produced by the generator function."
  },
  {
    "objectID": "projecteuler/multiplesof3or5.html#solution",
    "href": "projecteuler/multiplesof3or5.html#solution",
    "title": "Problem",
    "section": "",
    "text": "Let \\(A \\in \\mathbb{N}\\) be the set that contains all the natural numbers in the domain \\([1, 1000)\\) that are multiples of \\(3\\).\n\\[\\begin{align}\nA = 3, 6, 9, 12, 15, \\cdots, 999\n\\end{align}\\]\nLet \\(B \\in \\mathbb{N}\\) be the set that contains the numbers in the same domain as \\(A\\) but that are multiples of \\(5\\).\n\\[\\begin{align}\nB = 5, 10, 15, 20, 25, \\cdots, 995\n\\end{align}\\]\nEssentially what we are looking for here is the sum of all the elements contained in \\(A \\cup B\\).\nBy the inclusion-exculsion principle, recall that\n\\[\\begin{align}\nA \\cup B = A + B - A \\cap B\n\\end{align}\\]\nso we need to find the sum of the elements contained in \\(A\\), \\(B\\), and the set \\(C = A \\cap B\\) which is the set of all the numbers which are multiples of \\(15\\).\n\\[\\begin{align}\nC = 15, 30, 45, 60, \\cdots, 990\n\\end{align}\\]\nFor this we can use the formulas for Arithmetic Progression.\n\\[\\begin{align}\n\\sum A = n \\left(\\frac{a_1 + a_n}{2}\\right)\n\\end{align}\\]\nwhere \\(n\\) is the number of terms in the sequence, \\(a_1\\) is the first term and \\(a_n\\) is the last term. We don’t know what how many terms there are in the sequence so we can also use the nth term formula\n\\[\\begin{align}\na_n = a_1 + \\left(n - 1\\right)d\n\\end{align}\\]\nwhere \\(d\\) is the value being added to \\(a_{n-1}\\) to obtain \\(a_n\\).\nLet us compute these sums for \\(A, B, C\\)\n\n\nthe last term in the sequence is \\(999\\) thus,\n\\[\\begin{align}\n999 &= 3 + \\left(n - 1\\right)3\\\\\n999 &= 3n\\\\\nn &= 333\n\\end{align}\\]\nhence,\n\\[\\begin{align}\n\\sum A &= 333 \\left(\\frac{3 + 999}{2}\\right)\\\\\n&= 333 \\cdot \\frac{1002}{2} \\\\\n&= 166833\n\\end{align}\\]\n\n\n\nthe last term in the sequence is \\(995\\) thus,\n\\[\\begin{align}\n995 &= 5 + \\left(n - 1\\right)5\\\\\n995 &= 5n\\\\\nn &= 199\n\\end{align}\\]\nhence,\n\\[\\begin{align}\n\\sum B &= 199 \\left(\\frac{5 + 995}{2}\\right)\\\\\n&= 99500\n\\end{align}\\]\n\n\n\nthe last term in the sequence is \\(990\\) thus,\n\\[\\begin{align}\n990 &= 15 + \\left(n - 1\\right)15\\\\\n&= 15n\\\\\nn &= 66\n\\end{align}\\]\nhence,\n\\[\\begin{align}\n\\sum C &= 66 \\left(\\frac{15 + 990}{2}\\right)\\\\\n&= 66 \\cdot \\left(\\frac{1005}{2}\\right)\\\\\n&= 33165\n\\end{align}\\]\nTherefore the final solution,\n\\[\\begin{align}\n\\sum A \\cup B = 166833 + 99500 - 33165 = 233168\n\\end{align}\\]\n\n\n\n\nWe can solve the problem brute force by writing a code that computes the sum of any number in the range \\([1, 1000]\\) that is a multiple of \\(3\\) or \\(5\\).\n\nsum(x for x in 1:999 if (x % 3 == 0) || (x % 5 == 0))\n\n233168\n\n\nWe use a generator function in the call to sum so that we don’t generate an intermediate array and then sum over the elements of the array. Instead it simply keeps a running total of the elements produced by the generator function."
  },
  {
    "objectID": "projecteuler/largestpalindromeproduct.html",
    "href": "projecteuler/largestpalindromeproduct.html",
    "title": "Largest Palindrome Product",
    "section": "",
    "text": "A palindromic number reads the same both ways. The largest palindrome made from the product of two 2-digit numbers is\n\\[\\begin{align}\n9009 = 91 \\times 99\n\\end{align}\\]\nFind the largest palindrome made from the product of two 3-digit numbers.\n\n\n\nfunction isPalindrome(num::Int64)\n    x = Vector{Int}(undef, 18)\n    for i in eachindex(x)\n        x[i] = num % 10\n        num ÷= 10\n    end\n    y = x[1:findlast(i -&gt; i &gt; 0, x)]\n    for i in eachindex(y)\n        if !(y[i] == y[end - i + 1])\n            return false\n        end\n    end\n    return true\nend\n\nfunction main()\n    current = 0\n    idx = (0, 0)\n    for i in 999:-1:900\n        for j in 999:-1:900\n            new = i*j\n            if (new &gt; current && isPalindrome(new))\n                current = new\n                idx = (i, j)\n            end\n        end\n    end\n    println(\"$(current) = $(idx[1]) x $(idx[2])\")\nend\nmain()\n\n906609 = 993 x 913"
  },
  {
    "objectID": "projecteuler/largestpalindromeproduct.html#solution",
    "href": "projecteuler/largestpalindromeproduct.html#solution",
    "title": "Largest Palindrome Product",
    "section": "",
    "text": "function isPalindrome(num::Int64)\n    x = Vector{Int}(undef, 18)\n    for i in eachindex(x)\n        x[i] = num % 10\n        num ÷= 10\n    end\n    y = x[1:findlast(i -&gt; i &gt; 0, x)]\n    for i in eachindex(y)\n        if !(y[i] == y[end - i + 1])\n            return false\n        end\n    end\n    return true\nend\n\nfunction main()\n    current = 0\n    idx = (0, 0)\n    for i in 999:-1:900\n        for j in 999:-1:900\n            new = i*j\n            if (new &gt; current && isPalindrome(new))\n                current = new\n                idx = (i, j)\n            end\n        end\n    end\n    println(\"$(current) = $(idx[1]) x $(idx[2])\")\nend\nmain()\n\n906609 = 993 x 913"
  },
  {
    "objectID": "projecteuler/evenfibonaccinumbers.html",
    "href": "projecteuler/evenfibonaccinumbers.html",
    "title": "Even Fibonacci Numbers (Problem 2)",
    "section": "",
    "text": "Each new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with 1 and 2, the first 10 terms will be:\n\\[\\begin{align}\n1, 2, 3, 5, 8, 13, 21, 34, 55, 89, \\cdots\n\\end{align}\\]\nBy considering the terms in the Fibonacci sequence whose values do not exceed four million, find the sum of the even-valued terms.\n\n\nThe fibonacci sequence can be thought of as a discrete dynamical system or a recurrance relation defined as follows,\n\\[\\begin{align}\nx_{n+1} = x_{n} + x_{n-1}\n\\end{align}\\]\nwhere \\(x_{0} = 0\\) and \\(x_{1} = 1\\) and \\(n = 1, 2, 3, \\cdots, N\\).\nWhen \\(N = 35\\) the value \\(x_{N}\\) exceeds four million, thus we will solve for the fibonacci numbers up to the value \\(N = 34\\) and then use a julia filter function along with the iseven conditional function to sum only the even values.\n\nN = 34\nx = Vector{Int}(undef, N)\nx[1] = 0\nx[2] = 1\nfor n = 2:N-1\n    x[n+1] = x[n] + x[n-1]\nend\nprintln(sum(filter(iseven, x)))\n\n4613732"
  },
  {
    "objectID": "projecteuler/evenfibonaccinumbers.html#solution",
    "href": "projecteuler/evenfibonaccinumbers.html#solution",
    "title": "Even Fibonacci Numbers (Problem 2)",
    "section": "",
    "text": "The fibonacci sequence can be thought of as a discrete dynamical system or a recurrance relation defined as follows,\n\\[\\begin{align}\nx_{n+1} = x_{n} + x_{n-1}\n\\end{align}\\]\nwhere \\(x_{0} = 0\\) and \\(x_{1} = 1\\) and \\(n = 1, 2, 3, \\cdots, N\\).\nWhen \\(N = 35\\) the value \\(x_{N}\\) exceeds four million, thus we will solve for the fibonacci numbers up to the value \\(N = 34\\) and then use a julia filter function along with the iseven conditional function to sum only the even values.\n\nN = 34\nx = Vector{Int}(undef, N)\nx[1] = 0\nx[2] = 1\nfor n = 2:N-1\n    x[n+1] = x[n] + x[n-1]\nend\nprintln(sum(filter(iseven, x)))\n\n4613732"
  },
  {
    "objectID": "photos/index.html",
    "href": "photos/index.html",
    "title": "Travels",
    "section": "",
    "text": "Travels\nSpring Break 2025\nShasta 2023\n\n\nLife\n\n\nTue, 27 Oct 2015, 05:17:56 PM PDT\n\n\n\n\nMy first time at Thousand Island Lake. We hiked in from the trail head accross from silver lake. The plan was to summit Banner Peak, but we bailed half way up due to snow conditions."
  },
  {
    "objectID": "photos/springbreak2025/index.html",
    "href": "photos/springbreak2025/index.html",
    "title": "Spring Break 2025",
    "section": "",
    "text": "Spring Break 2025\n\n\nSun, 23 Mar 2025, 03:08:16 AM PDT\n\n\n\n\nWe found our friends! The view from our camp site looking at Pine creek canyon.\n\n\n\n\n\nSun, 23 Mar 2025, 05:47:38 AM PDT\n\n\n\n\nGetting our gear ready for the hike up to the climb.\n\n\n\n\n\nSun, 23 Mar 2025, 05:48:42 AM PDT\n\n\n\n\nThe view of the crag from the road.\n\n\n\n\n\nSun, 23 Mar 2025, 06:05:14 AM PDT\n\n\n\n\nLooking out accross the canyon. We found a cairn!\n\n\n\n\n\nSun, 23 Mar 2025, 07:15:12 AM PDT\n\n\n\n\nAbout to rope up and climb some slab! Kaela found the belay spot.\n\n\n\n\n\nSun, 23 Mar 2025, 07:25:04 AM PDT\n\n\n\n\nCrushing the scarry slab climb!\n\n\n\n\n\nSun, 23 Mar 2025, 01:17:44 PM PDT\n\n\n\n\nRight before sunset at the camp after we got back from Pine Creek.\n\n\n\nI bought some snow shoes at the Mammoth Gear Exchange for 15 dollars and decided to take a rest day from climbing. I drove up to aspendell and snowshoed to Lake Sabrina!\n\n\nMon, 24 Mar 2025, 11:30:20 AM PDT\n\n\n\n\nLake Sabrina still frozen.\n\n\n\n\n\nWed, 26 Mar 2025, 01:10:16 PM PDT\n\n\n\n\nBishop Sunset.\n\n\n\n\n\nThu, 27 Mar 2025, 02:11:48 AM PDT\n\n\n\n\nObligatory group photo :)\n\n\n\n\n\nThu, 27 Mar 2025, 02:51:46 PM PDT\n\n\n\n\nMy beautiful girl friend Kaela Rose. We visited my mom after our Bishop trip.\n\n\n\nback to top"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "paxnomada",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Kevin Andrew Silberberg\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  }
]